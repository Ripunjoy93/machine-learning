{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purpose\n",
    "This is a reimplementation of https://pytorch.org/tutorials/beginner/transformer_tutorial.html using pytorch-lightening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "import math\n",
    "\n",
    "from argparse import ArgumentParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refer to https://github.com/achinta/machine-learning/blob/master/notebooks/nlp/torchtext-tutorial.ipynb\n",
    "class LMDataset(Dataset):\n",
    "    def __init__(self, data, bptt, bsz):\n",
    "        '''\n",
    "        data is a tensor of shape [k,1], where k is the number of words in text\n",
    "        '''\n",
    "        self.bptt = bptt\n",
    "        self.bsz = bsz\n",
    "        \n",
    "        # Divide the dataset into bsz parts.\n",
    "        nbatches = data.size(0)//bsz\n",
    "        \n",
    "        # Trim off any extra elements that wouldn't cleanly fit (remainders).\n",
    "        data = data.narrow(dim=0, start=0, length=nbatches*bsz)\n",
    "        \n",
    "        # Evenly divide the data across the bsz batches.\n",
    "        self.data = data.view(bsz, -1).t().contiguous()\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        data = self.data[i:i+self.bptt]\n",
    "        target = self.data[i+1:i+1+self.bptt].view(-1)\n",
    "        return data, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return(self.data.size(0) - self.bptt)\n",
    "    \n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(LMModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.src_mask = None\n",
    "        self.pos_encoder = PositionalEncoding(hparams.ninp, hparams.dropout)\n",
    "        encoder_layers = nn.TransformerEncoderLayer(hparams.ninp, hparams.nhead, hparams.nhid, hparams.dropout)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, hparams.nlayers)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.src_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_model_specifi_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('-bsz', default=20, type=int, help='batch_size', )\n",
    "        parser.add_argument('-bptt', default=35, type=int, help='sentence length')\n",
    "        parser.add_argument('-ninp', default=256, type=int, help='expected features in the input')\n",
    "        parser.add_argument('-nhead', default=4, type=int, help='number of attention heads')\n",
    "        parser.add_argument('-nhid', default=1024, type=int, help='dimesion of feed-forward network model')\n",
    "        parser.add_argument('-nlayers', default=3, type=int, help='number of encoder layers')\n",
    "        parser.add_argument('-dropout', default=0.2, type=float, help='dropout')\n",
    "        return parser\n",
    "    \n",
    "    \n",
    "    def _generate_square_subsequent_mask(self, sz):\n",
    "        # populate the lower triangle with True and rest with False\n",
    "        return torch.tril(torch.ones(sz, sz)) == 1.0\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        self.field = torchtext.data.Field(tokenize=get_tokenizer('basic_english'),\n",
    "                                    init_token='<sos>',\n",
    "                                    eos_token='<eos>',\n",
    "                                    lower=True)\n",
    "        self.train_txt, self.val_txt, self.test_txt = torchtext.datasets.WikiText2.splits(self.field)\n",
    "        self.field.build_vocab(self.train_txt)\n",
    "        \n",
    "        # create source embedding\n",
    "        self.ntoken = len(self.field.vocab)\n",
    "        self.src_embedding = nn.Embedding(self.ntoken, self.hparams.ninp)\n",
    "        self.decoder = nn.Linear(self.hparams.ninp, self.ntoken)\n",
    "        self.init_weights()\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_data = self.field.numericalize([self.train_txt.examples[0].text])\n",
    "        train_ds = LMDataset(train_data,self.hparams.bptt, self.hparams.bsz )\n",
    "        return DataLoader(train_ds, shuffle=True)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "            device = src.device\n",
    "            mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "            self.src_mask = mask\n",
    "            \n",
    "        src = self.src_embedding(src) * math.sqrt(self.hparams.ninp)\n",
    "        src = self.pos_encoder(src)\n",
    "        output = self.transformer_encoder(src, self.src_mask)\n",
    "        output = self.decoder(output)\n",
    "        return output\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        x = x.squeeze(0)\n",
    "        y = y.squeeze(0)\n",
    "        yhat = self(x)\n",
    "#         print(f'x.shape - {x.shape} yhat.shape - {yhat.shape}  \\t y.shape - {y.shape}')\n",
    "        loss = self.criterion(yhat.reshape(-1, self.ntoken), y)\n",
    "        return {'loss': loss}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd327bfd3cb54bb7a715321e7b2a5793",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = ArgumentParser()\n",
    "parser = LMModel.add_model_specifi_args(parser)\n",
    "hparams = parser.parse_args(\"\")\n",
    "lm = LMModel(hparams)\n",
    "\n",
    "trainer = Trainer()\n",
    "trainer.fit(lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2086708, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "521674"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "field = torchtext.data.Field(tokenize=get_tokenizer('basic_english'),\n",
    "                            init_token='<sos>',\n",
    "                            eos_token='<eos>',\n",
    "                            lower=True)\n",
    "train_txt, val_txt, test_txt = torchtext.datasets.WikiText2.splits(field)\n",
    "\n",
    "field.build_vocab(train_txt)\n",
    "\n",
    "data = field.numericalize([train_txt.examples[0].text])\n",
    "data.shape\n",
    "\n",
    "bptt = 3\n",
    "bsz = 4\n",
    "train_data = field.numericalize([train_txt.examples[0].text])\n",
    "print(train_data.shape)\n",
    "train_ds = LMDataset(train_data,bptt, bsz )\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28785"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lm.field.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
