{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# !pip install pytorch_lightning\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "data_dir = Path.home()/'data/kaggle/m5-forecasting-accuracy'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    " - normalize y\n",
    " - sales price is 0. fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales.shape: (30490, 1919)\n",
      "CPU times: user 7.4 s, sys: 4.19 s, total: 11.6 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sales = pd.read_csv(data_dir/'sales_train_validation.csv')\n",
    "print(f'sales.shape: {sales.shape}')\n",
    "cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "# encode cat cols\n",
    "encoders = {}\n",
    "for col in cat_cols:\n",
    "    encoder =  OrdinalEncoder()\n",
    "    sales[[col]] = encoder.fit_transform(sales[[col]])\n",
    "    sales[col] = sales[col].astype(np.long)\n",
    "    encoders[col] = encoder\n",
    "    \n",
    "# change day column names to just day number\n",
    "day_cols = {col: col.split('_')[1] for col in sales.columns if col.startswith('d_')}\n",
    "sales.rename(columns=day_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num store_items -  30490\n"
     ]
    }
   ],
   "source": [
    "num_days = len(day_cols)\n",
    "num_stores = sales['store_id'].nunique()\n",
    "num_items = sales['item_id'].nunique()\n",
    "print('num store_items - ', num_stores * num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['item_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(data_dir/'calendar.csv')\\\n",
    "            .rename(columns={'d':'day'})\n",
    "\n",
    "cat_cal_cols = ['wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n",
    "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "       'snap_CA', 'snap_TX', 'snap_WI']\n",
    "# ignore_cal_cols = ['wm_yr_wk']\n",
    "\n",
    "for col in cat_cal_cols:\n",
    "    \n",
    "    # impute\n",
    "    if str(calendar[col].dtype)[:3] == 'obj':\n",
    "        fill_value = 'abcxyz' \n",
    "    elif str(calendar[col].dtype)[:3] == 'int':\n",
    "        fill_value = -1\n",
    "    calendar[[col]] = SimpleImputer(strategy='constant', fill_value=fill_value).fit_transform(calendar[[col]])\n",
    "    \n",
    "    # encode\n",
    "    if col not in encoders:\n",
    "        encoders[col] = OrdinalEncoder().fit(calendar[[col]])\n",
    "    calendar[[col]] = encoders[col].transform(calendar[[col]])\n",
    "    calendar[col] = calendar[col].astype(np.long)\n",
    "    \n",
    "# change day column names to just day number\n",
    "calendar['day'] = calendar['day'].apply(lambda x: x.split('_')[1])\n",
    "calendar['day'] = calendar['day'].astype(np.long)\n",
    "\n",
    "calendar.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prices = pd.read_csv(data_dir/'sell_prices.csv')\n",
    "for col in ['store_id', 'item_id', 'wm_yr_wk']:\n",
    "    prices[[col]] = encoders[col].transform(prices[[col]])\n",
    "    prices[col] = prices[col].astype(np.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales2 = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                                       var_name='day', value_name='demand')\n",
    "sales2['day'] = sales2['day'].astype(np.long)\n",
    "\n",
    "sales2.sort_values('day', inplace=True)\n",
    "calendar.sort_values('day', inplace=True)\n",
    "\n",
    "sales2 = sales2.merge(calendar, on='day', how='left')\n",
    "sales2 = sales2.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "sales2['sell_price'] = sales2['sell_price'].astype(np.float32)\n",
    "sales2['sell_price'] = sales2['sell_price'].fillna(0.0)\n",
    "\n",
    "sales2.sort_values(['item_id', 'store_id','day'], inplace=True)\n",
    "\n",
    "sales2.to_parquet('combined.pq')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales2 = pd.read_parquet('combined.pq')\n",
    "print(sales2.shape)\n",
    "sales2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4 µs, sys: 9 µs, total: 13 µs\n",
      "Wall time: 26.5 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'day',\n",
    "        'weekday', 'wday', 'month', 'year',\n",
    "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "       'snap_CA', 'snap_TX', 'snap_WI']\n",
    "x_cont_cols = ['sell_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# sales2.sort_values(['item_id', 'store_id','day'], inplace=True)\n",
    "sales2['sell_price'] = sales2['sell_price'].fillna(0.0)\n",
    "sales2['sell_price'] = sales2['sell_price'].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = torch.tensor(sales2[x_cat_cols + x_cont_cols].values)\n",
    "y = torch.tensor(sales2['demand'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales2[x_cat_cols + x_cont_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# from fastai v2\n",
    "def get_emb_size(nunique):\n",
    "    return min(600, round(1.6 * nunique**0.56))\n",
    "\n",
    "emb_sizes = [(sales2[col].nunique(), get_emb_size(sales2[col].nunique())) for col in x_cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb_sz.pkl','wb') as f:\n",
    "    pickle.dump(emb_sizes,f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_size = num_items * num_stores\n",
    "group_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_features = x.size(1)\n",
    "x1 = x.view(-1, num_days, num_features).refine_names('item_store', 'day','features')\\\n",
    "        .align_to('day','item_store','features').contiguous()\n",
    "\n",
    "y1 = y.view(-1, num_days).refine_names('item_store', 'day')\\\n",
    "    .align_to('day', 'item_store').contiguous()\n",
    "\n",
    "print(f'x1.shape - {x1.shape} y1.shape - {y1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.save(x1.rename(None), 'x.pt')\n",
    "torch.save(y1.rename(None), 'y.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.unique(x1.rename(None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5DataSet(Dataset):\n",
    "    def __init__(self,x, y, src_len, tgt_len, bsz):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.src_len = src_len\n",
    "        self.tgt_len = tgt_len\n",
    "        self.bsz = bsz\n",
    "        \n",
    "    def __len__(self):\n",
    "        l = (self.x.size(0) - (self.src_len + self.tgt_len)) \n",
    "        return l\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # we have 30490 item_stores. We may not be able to load them all. So randomly pick bsz items. \n",
    "        item_store_ids = list(np.random.randint(0, self.x.size(1),(self.bsz,)))\n",
    "        \n",
    "        x_src = self.x.rename(None)[idx:idx+self.src_len, item_store_ids, :]\n",
    "        x_tgt = self.x.rename(None)[idx+self.src_len:idx+self.src_len+self.tgt_len, item_store_ids, :]\n",
    "        y_src = self.y.rename(None)[idx:idx+self.src_len, item_store_ids]\n",
    "        y_tgt = self.y.rename(None)[idx+self.src_len:idx+self.src_len+self.tgt_len, item_store_ids]\n",
    "#         print(f'x.shape - {self.x.shape} y.shape - {self.y.shape} idx - {idx}. x_item.shape - {x_item.shape} y_item.shape - {y_item.shape}')\n",
    "        return x_src, x_tgt, y_src, y_tgt\n",
    "\n",
    "# train_ds = M5DataSet(x1, y1, src_len, tgt_len, 200)\n",
    "# train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dl' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dl' is not defined"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "x_b, y_b = next(iter(train_dl))\n",
    "bsz = 100\n",
    "print(f'x_b.shape - {x_b.shape}, y_b.shape - {y_b.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training using lightening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "def insert_embedding(inp, dim, index, emb):\n",
    "    \"\"\"\n",
    "    Replace columns with their embeddings. Works only with 2-d tensors.\n",
    "    TODO - make it work for multi-dim tensors\n",
    "\n",
    "    :param inp: tensor of two or more dimensions\n",
    "    :param dim: dimension along which tensor should be expanded by inserting the embedding\n",
    "    :param i: index of tensor along dim which is to be embedded\n",
    "    :param emb: Embedding of shape [v,d], where v vocab_size and d is embedding dimension\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    # create a slice of the data to be replaced with embedding. \n",
    "    s = inp.index_select(dim, torch.tensor([index])).squeeze(dim)\n",
    "    embedded = emb(s.type(torch.long))\n",
    "    \n",
    "    first_indices = torch.arange(0,index)\n",
    "    last_indices = torch.arange(index+1,inp.size(dim))\n",
    "\n",
    "    return torch.cat([inp.index_select(dim, first_indices), embedded.type(inp.dtype), inp.index_select(dim, last_indices)], axis=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gx = torch.load('x.pt')\n",
    "gy = torch.load('y.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesModel(LightningModule):\n",
    "    def __init__(self, hparams, x_cat_cols, x_cont_cols):\n",
    "        super(SalesModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.x_cat_cols = x_cat_cols\n",
    "        self.x_cont_cols = x_cont_cols\n",
    "        self.pos_encoder = PositionalEncoding(hparams.ninp, hparams.dropout)\n",
    "#         encoder_layers = nn.TransformerEncoderLayer(hparams.ninp, hparams.nhead, hparams.nhid, hparams.dropout)\n",
    "#         decoder_layers = nn.TransformerDecoderLayer(hparams.ninp, hparams.nhead, hparams.nhid, hparams.dropout)\n",
    "#         self.transformer_encoder = nn.TransformerEncoder(encoder_layers, hparams.nlayers)\n",
    "#         self.transformer_decoder = nn.TransformerDecoder(decoder_layers, hparams.nlayers)\n",
    "#         self.lin = nn.Linear()\n",
    "        self.transformer = nn.Transformer(nhead=hparams.nhead, num_encoder_layers=hparams.nlayers)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        \n",
    "#     def init_weights(self):\n",
    "#         initrange = 0.1\n",
    "#         self.src_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "#         self.decoder.bias.data.zero_()\n",
    "#         self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_model_specifi_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--bsz', default=20, type=int, help='batch_size', )\n",
    "        parser.add_argument('--src-len', default=90, type=int, help='source length')\n",
    "        parser.add_argument('--tgt-len', default=28, type=int, help='target length')\n",
    "        parser.add_argument('--ninp', default=320, type=int, help='expected features in the input')\n",
    "        parser.add_argument('--nhead', default=4, type=int, help='number of attention heads')\n",
    "        parser.add_argument('--nhid', default=1024, type=int, help='dimesion of feed-forward network model')\n",
    "        parser.add_argument('--nlayers', default=2, type=int, help='number of encoder layers')\n",
    "        parser.add_argument('--dropout', default=0.2, type=float, help='dropout')\n",
    "        return parser\n",
    "    \n",
    "#     def _generate_square_subsequent_mask(self, sz):\n",
    "#         # populate the lower triangle with True and rest with False\n",
    "#         return torch.tril(torch.ones(sz, sz)) == 1.0\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        print('reading data', flush=True)\n",
    "        self.x = gx\n",
    "        self.y = gy\n",
    "\n",
    "        \n",
    "        with open('emb_sz.pkl','rb') as f:\n",
    "            emb_szs = pickle.load(f)\n",
    "        print(f'emb_szs - {emb_szs}')\n",
    "                    \n",
    "        self.embs = nn.ModuleList([nn.Embedding(e[0],e[1]) for e in emb_szs])\n",
    "\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_ds = M5DataSet(self.x, self.y, self.hparams.src_len, self.hparams.tgt_len, self.hparams.bsz)        \n",
    "        train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, pin_memory=True)\n",
    "        return train_dl\n",
    "\n",
    "    def forward(self, x_src, y_src, x_tgt):\n",
    "        offset = 0\n",
    "        embs_t = []\n",
    "        for idx, col in enumerate(self.x_cat_cols):\n",
    "             embs_t.append(self.embs[idx](x_src[:,:,idx].type(torch.long)))\n",
    "        x_src_cat = torch.cat(embs_t, dim=2)\n",
    "        x_src_cont = x_src[:,:,len(x_cat_cols):]\n",
    "        x_src = torch.cat([x_src_cat, x_src_cont.type(x_src_cat.dtype), y_src.unsqueeze(2)], dim=2)\n",
    "        print(f'x_src_cat - {x_src_cat.shape}')\n",
    "        print(f'x_src_cnt - {x_src_cont.shape}')\n",
    "        print(f'x_src - {x_src.shape}')\n",
    "        print(f'y.shape - {y_src.shape}')\n",
    "#         for x in [x_src, x_tgt]:\n",
    "#             for idx, col in enumerate(self.x_cat_cols):\n",
    "#     #             print(f'emb for col - {idx}, {col}, {self.embs[idx]} inserting at {offset}')\n",
    "#                 x = insert_embedding(x, dim=2, index=offset, emb=self.embs[idx])\n",
    "#                 offset += self.embs[idx].weight.size(1)\n",
    "            \n",
    "        print(f'shape after x_src: {x_src.shape}, x_tgt.shape: {x_tgt.shape}')\n",
    "            \n",
    "        #pad to adjust the feature dimension\n",
    "        dim3_shortfall = self.hparams.ninp - x.size(2)\n",
    "        assert dim3_shortfall >= 0\n",
    "        pad = nn.ConstantPad1d(padding=(0,1),value=0)\n",
    "        x = pad(x)\n",
    "        \n",
    "        x = self.pos_encoder(x)\n",
    "        print('shape after pos encoder - ', x.size())\n",
    "        x = self.transformer(x)\n",
    "        print('shape after transformer - ', x.size())\n",
    "        \n",
    "#         if self.src_mask is None or self.src_mask.size(0) != len(src):\n",
    "#             device = src.device\n",
    "#             mask = self._generate_square_subsequent_mask(len(src)).to(device)\n",
    "#             self.src_mask = mask\n",
    "            \n",
    "#         src = self.src_embedding(src) * math.sqrt(self.hparams.ninp)\n",
    "#         src = self.pos_encoder(src)\n",
    "#         output = self.transformer_decoder(self.transformer_encoder(src))\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_src, x_tgt, y_src, y_tgt = batch\n",
    "        x_src = x_src.squeeze(0)\n",
    "        x_tgt = x_tgt.squeeze(0)\n",
    "        y_src = y_src.squeeze(0)\n",
    "        y_tgt = y_tgt.squeeze(0)\n",
    "        \n",
    "        print(f'x_src.shape - {tuple(x_src.shape)} \\t x_tgt.shape - {tuple(x_tgt.shape)} \\t y_src.shape - {y_src.shape} \\t y_tgt.shape - {y_tgt.shape}')\n",
    "        yhat = self(x_src, y_src, x_tgt)\n",
    "        print(f'yhat.shape - {yhat.shape}')\n",
    "        loss = self.criterion(yhat.reshape(-1, self.ntoken), y)\n",
    "        return {'loss': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n",
      "emb_szs - [(3049, 143), (7, 5), (3, 3), (10, 6), (3, 3), (1913, 110), (7, 5), (7, 5), (12, 6), (6, 4), (31, 11), (5, 4), (5, 4), (3, 3), (2, 2), (2, 2), (2, 2)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8171e9998c87461ca3b3ec82d1d3d21f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(flex='2'), max=1.0), HTML(value='')), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_src.shape - torch.Size([90, 200, 18]) \t x_tgt.shape - torch.Size([28, 200, 18]) \t y_src.shape - torch.Size([90, 200]) \t y_tgt.shape - torch.Size([28, 200])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of scalar type float but got scalar type long int for sequence element 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-b0d25f12fc0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSalesModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cat_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cont_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, test_dataloaders)\u001b[0m\n\u001b[1;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_schedulers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfigure_optimizers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 630\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_pretrain_routine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    631\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m         \u001b[0;31m# return 1 when finished\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mrun_pretrain_routine\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    829\u001b[0m         \u001b[0;31m# CORE TRAINING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 830\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mLightningModule\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0;31m# RUN TNG EPOCH\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m                 \u001b[0;31m# -----------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 343\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    345\u001b[0m                 \u001b[0;31m# update LR schedulers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_epoch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0;31m# RUN TRAIN STEP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;31m# ---------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_training_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0mbatch_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm_dic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_step_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mrun_training_batch\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m                 \u001b[0;31m# calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer_closure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;31m# nan grads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36moptimizer_closure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    527\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_forward'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m                         output = self.training_forward(\n\u001b[0;32m--> 529\u001b[0;31m                             split_batch, batch_idx, opt_idx, self.hiddens)\n\u001b[0m\u001b[1;32m    530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m                     \u001b[0mclosure_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/pytorch_lightning/trainer/training_loop.py\u001b[0m in \u001b[0;36mtraining_forward\u001b[0;34m(self, batch, batch_idx, opt_idx, hiddens)\u001b[0m\n\u001b[1;32m    680\u001b[0m         \u001b[0;31m# CPU forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m         \u001b[0;31m# allow any mode to define training_step_end\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-4f70ab42d67f>\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'x_src.shape - {x_src.shape} \\t x_tgt.shape - {x_tgt.shape} \\t y_src.shape - {y_src.shape} \\t y_tgt.shape - {y_tgt.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0myhat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_src\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_tgt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'yhat.shape - {yhat.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myhat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mntoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    548\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 550\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    551\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    552\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-4f70ab42d67f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x_src, y_src, x_tgt)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mx_src_cat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membs_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mx_src_cont\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_src\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cat_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mx_src\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_src_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_src_cont\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_src_cat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_src\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'x_src_cat - {x_src_cat.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'x_src_cnt - {x_src_cont.shape}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of scalar type float but got scalar type long int for sequence element 2."
     ]
    }
   ],
   "source": [
    "src_len = 90\n",
    "tgt_len = 28\n",
    "bsz = 200\n",
    "# model = SalesModel(hparams)\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser = SalesModel.add_model_specifi_args(parser)\n",
    "hparams = parser.parse_args('--bsz 200 --ninp 320'.split())\n",
    "\n",
    "model = SalesModel(hparams, x_cat_cols, x_cont_cols)\n",
    "trainer = Trainer()\n",
    "trainer.fit(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales2[sales2.sell_price > 0]['sell_price']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1., 1., 1., 0.],\n",
       "         [1., 1., 1., 0.],\n",
       "         [1., 1., 1., 0.]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.ConstantPad1d(padding=(0,1),value=0)\n",
    "t = torch.ones((1,3,3))\n",
    "m(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1913, 30490, 18)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(gx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__contains__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmul__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " 'count',\n",
       " 'index',\n",
       " 'numel']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(gx.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
