{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# !pip install --upgrade pytorch_lightning\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# !pip install wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "\n",
    "data_dir = Path.home()/'data/kaggle/m5-forecasting-accuracy'\n",
    "\n",
    "x_cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
    "        'weekday', 'wday', 'month', 'year',\n",
    "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "       'snap_CA', 'snap_TX', 'snap_WI']\n",
    "x_cont_cols = ['sell_price']\n",
    "num_train_val_days = 1913\n",
    "num_test1_days = 28\n",
    "num_test2_days = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    " - normalize y\n",
    " - sales price is 0. fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sales.shape: (30490, 1919)\n",
      "CPU times: user 7.73 s, sys: 4.43 s, total: 12.2 s\n",
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sales = pd.read_csv(data_dir/'sales_train_validation.csv')\n",
    "print(f'sales.shape: {sales.shape}')\n",
    "cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "# encode cat cols\n",
    "encoders = {}\n",
    "for col in cat_cols:\n",
    "    encoder =  OrdinalEncoder()\n",
    "    sales[[col]] = encoder.fit_transform(sales[[col]])\n",
    "    sales[col] = sales[col].astype(np.long)\n",
    "    encoders[col] = encoder\n",
    "    \n",
    "# change day column names to just day number\n",
    "train_day_cols = {col: col.split('_')[1] for col in sales.columns if col.startswith('d_')}\n",
    "sales.rename(columns=train_day_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_day_cols = [str(num_train_val_days + 1 + o) for o in range(56)]\n",
    "for col in test_day_cols:\n",
    "    sales[col] = 0\n",
    "print(sales.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(data_dir/'sample_submission.csv')\n",
    "sample.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = len(train_day_cols) + len(test_day_cols)\n",
    "num_stores = sales['store_id'].nunique()\n",
    "num_items = sales['item_id'].nunique()\n",
    "print('total days : ', num_days)\n",
    "print('num store_items - ', num_stores * num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['item_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(data_dir/'calendar.csv')\\\n",
    "            .rename(columns={'d':'day'})\n",
    "\n",
    "cat_cal_cols = ['wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n",
    "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "       'snap_CA', 'snap_TX', 'snap_WI']\n",
    "# ignore_cal_cols = ['wm_yr_wk']\n",
    "\n",
    "for col in cat_cal_cols:\n",
    "    \n",
    "    # impute\n",
    "    if str(calendar[col].dtype)[:3] == 'obj':\n",
    "        fill_value = 'abcxyz' \n",
    "    elif str(calendar[col].dtype)[:3] == 'int':\n",
    "        fill_value = -1\n",
    "    calendar[[col]] = SimpleImputer(strategy='constant', fill_value=fill_value).fit_transform(calendar[[col]])\n",
    "    \n",
    "    # encode\n",
    "    if col not in encoders:\n",
    "        encoders[col] = OrdinalEncoder().fit(calendar[[col]])\n",
    "    calendar[[col]] = encoders[col].transform(calendar[[col]])\n",
    "    calendar[col] = calendar[col].astype(np.long)\n",
    "    \n",
    "# change day column names to just day number\n",
    "calendar['day'] = calendar['day'].apply(lambda x: x.split('_')[1])\n",
    "calendar['day'] = calendar['day'].astype(np.long)\n",
    "\n",
    "calendar.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prices = pd.read_csv(data_dir/'sell_prices.csv')\n",
    "for col in ['store_id', 'item_id', 'wm_yr_wk']:\n",
    "    prices[[col]] = encoders[col].transform(prices[[col]])\n",
    "    prices[col] = prices[col].astype(np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.sort_values('wm_yr_wk',ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales2 = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                                       var_name='day', value_name='demand')\n",
    "sales2['day'] = sales2['day'].astype(np.long)\n",
    "\n",
    "sales2.sort_values('day', inplace=True)\n",
    "calendar.sort_values('day', inplace=True)\n",
    "\n",
    "sales2 = sales2.merge(calendar, on='day', how='left')\n",
    "sales2 = sales2.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "sales2['sell_price'] = sales2['sell_price'].astype(np.float32)\n",
    "sales2['sell_price'] = sales2['sell_price'].fillna(0.0)\n",
    "\n",
    "sales2.sort_values(['item_id', 'store_id','day'], inplace=True)\n",
    "\n",
    "# scale continuous columns\n",
    "scalers = {}\n",
    "for col in ['sell_price','demand']:\n",
    "    scaler = MinMaxScaler()\n",
    "    sales2[[col]] = scaler.fit_transform(sales2[[col]])\n",
    "    scalers[col] = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales2.to_parquet('combined.pq')\n",
    "with open('encoders.pkl','wb') as f:\n",
    "    pickle.dump(encoders,f)\n",
    "    \n",
    "with open('scalers.pkl','wb') as f:\n",
    "    pickle.dump(scalers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales2 = pd.read_parquet('combined.pq')\n",
    "print(sales2.shape)\n",
    "sales2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = torch.tensor(sales2[x_cat_cols + x_cont_cols].values)\n",
    "y = torch.tensor(sales2['demand'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales2[x_cat_cols + x_cont_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# from fastai v2\n",
    "def get_emb_size(nunique):\n",
    "    return min(600, round(1.6 * nunique**0.56))\n",
    "\n",
    "emb_sizes = [(sales2[col].nunique(), get_emb_size(sales2[col].nunique())) for col in x_cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb_sz.pkl','wb') as f:\n",
    "    pickle.dump(emb_sizes,f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_size = num_items * num_stores\n",
    "# group_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_features = x.size(1)\n",
    "x1 = x.view(-1, num_days, num_features).refine_names('item_store', 'day','features')\\\n",
    "        .align_to('day','item_store','features').contiguous()\n",
    "\n",
    "y1 = y.view(-1, num_days).refine_names('item_store', 'day')\\\n",
    "    .align_to('day', 'item_store').contiguous()\n",
    "\n",
    "print(f'x1.shape - {x1.shape} y1.shape - {y1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.save(x1.rename(None), 'x.pt')\n",
    "torch.save(y1.rename(None), 'y.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5DataSet(Dataset):\n",
    "    def __init__(self,x, y, src_len, tgt_len, bsz, dstype='train'):\n",
    "        assert dstype in ['train', 'test1', 'test2', 'val']\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.src_len = src_len\n",
    "        self.tgt_len = tgt_len\n",
    "        self.bsz = bsz\n",
    "        self.dstype = dstype\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.dstype == 'train':\n",
    "            l = (self.x.size(0) - (self.src_len + self.tgt_len + num_test1_days + num_test2_days)) \n",
    "            return l\n",
    "        \n",
    "        if self.dstype == 'test1':\n",
    "            return 1\n",
    "        \n",
    "        return l\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.dstype == 'train':\n",
    "            # we have 30490 item_stores. We may not be able to load them all. So randomly pick bsz items. \n",
    "            item_store_mask = list(np.random.randint(0, self.x.size(1),(self.bsz,)))\n",
    "        elif self.dstype == 'test1':\n",
    "            item_store_mask = list(np.arange(self.x.size(1)))\n",
    "            idx = self.x.size(0) - (self.src_len + self.tgt_len + num_test2_days)\n",
    "            print('test1 index - ', idx)\n",
    "        \n",
    "        x_src = self.x.rename(None)[idx:idx+self.src_len, item_store_mask, :]\n",
    "        x_tgt = self.x.rename(None)[idx+self.src_len:idx+self.src_len+self.tgt_len, item_store_mask, :]\n",
    "        y_src = self.y.rename(None)[idx:idx+self.src_len, item_store_mask]\n",
    "        y_tgt = self.y.rename(None)[idx+self.src_len:idx+self.src_len+self.tgt_len, item_store_mask]\n",
    "#         print(f'x.shape - {self.x.shape} y.shape - {self.y.shape} idx - {idx}. x_item.shape - {x_item.shape} y_item.shape - {y_item.shape}')\n",
    "        return x_src, x_tgt, y_src, y_tgt, item_store_mask\n",
    "\n",
    "# train_ds = M5DataSet(x1, y1, src_len, tgt_len, 200)\n",
    "# train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# def insert_embedding(inp, dim, index, emb):\n",
    "#     \"\"\"\n",
    "#     Replace columns with their embeddings. Works only with 2-d tensors.\n",
    "#     TODO - make it work for multi-dim tensors\n",
    "\n",
    "#     :param inp: tensor of two or more dimensions\n",
    "#     :param dim: dimension along which tensor should be expanded by inserting the embedding\n",
    "#     :param i: index of tensor along dim which is to be embedded\n",
    "#     :param emb: Embedding of shape [v,d], where v vocab_size and d is embedding dimension\n",
    "#     :return: \n",
    "#     \"\"\"\n",
    "#     # create a slice of the data to be replaced with embedding. \n",
    "#     s = inp.index_select(dim, torch.tensor([index])).squeeze(dim)\n",
    "#     embedded = emb(s.type(torch.long))\n",
    "    \n",
    "#     first_indices = torch.arange(0,index)\n",
    "#     last_indices = torch.arange(index+1,inp.size(dim))\n",
    "\n",
    "#     return torch.cat([inp.index_select(dim, first_indices), embedded.type(inp.dtype), inp.index_select(dim, last_indices)], axis=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gx.shape - torch.Size([1969, 30490, 17]) gy.shape - torch.Size([1969, 30490])\n",
      "CPU times: user 3.92 ms, sys: 7.46 s, total: 7.46 s\n",
      "Wall time: 7.55 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gx = torch.load('x.pt')\n",
    "gy = torch.load('y.pt')\n",
    "print(f'gx.shape - {gx.shape} gy.shape - {gy.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(SalesModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.x_cat_cols = x_cat_cols\n",
    "        self.x_cont_cols = x_cont_cols\n",
    "        self.pos_encoder = PositionalEncoding(hparams.ninp, hparams.dropout)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(hparams.ninp, hparams.nhead, hparams.nhid, hparams.dropout)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(hparams.ninp, hparams.nhead, hparams.nhid, hparams.dropout)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layers, hparams.nlayers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layers, hparams.nlayers)\n",
    "        \n",
    "#         self.lin = nn.Linear()\n",
    "#         self.transformer = nn.Transformer(d_model=hparams.ninp, nhead=hparams.nhead, \n",
    "#                                           num_encoder_layers=hparams.nlayers,\n",
    "#                                           num_decoder_layers=hparams.nlayers,\n",
    "#                                           dim_feedforward=hparams.nhid)\n",
    "        self.transformer = nn.Transformer(d_model=hparams.ninp,\n",
    "                                          custom_encoder=self.encoder, \n",
    "                                          custom_decoder = self.decoder)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.lin1 = nn.Linear(hparams.ninp, 50)\n",
    "        self.lin2 = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        print('reading data', flush=True)\n",
    "        self.x = gx\n",
    "        self.y = gy\n",
    "\n",
    "        with open('emb_sz.pkl','rb') as f:\n",
    "            emb_szs = pickle.load(f)\n",
    "        print(f'emb_szs - {emb_szs}')\n",
    "                    \n",
    "        self.embs = nn.ModuleList([nn.Embedding(e[0],e[1]) for e in emb_szs])\n",
    "#         self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "#         self.src_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_model_specifi_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--bsz', default=20, type=int, help='batch_size', )\n",
    "        parser.add_argument('--src-len', default=90, type=int, help='source length')\n",
    "        parser.add_argument('--tgt-len', default=28, type=int, help='target length')\n",
    "        parser.add_argument('--ninp', default=320, type=int, help='expected features in the input')\n",
    "        parser.add_argument('--nhead', default=4, type=int, help='number of attention heads')\n",
    "        parser.add_argument('--nhid', default=256, type=int, help='dimesion of feed-forward network model')\n",
    "        parser.add_argument('--nlayers', default=2, type=int, help='number of encoder layers')\n",
    "        parser.add_argument('--dropout', default=0.2, type=float, help='dropout')\n",
    "        \n",
    "        # they are not hyper params, but adding them as pytorch lightening can save them\n",
    "        parser.add_argument('--num-cat-cols', default=len(x_cat_cols), type=int, help='number of categorical columns')\n",
    "        parser.add_argument('--num-cont-cols', default=len(x_cont_cols), type=int, help='number of numeric columns')\n",
    "        return parser\n",
    "    \n",
    "#     def _generate_square_subsequent_mask(self, sz):\n",
    "#         # populate the lower triangle with True and rest with False\n",
    "#         return torch.tril(torch.ones(sz, sz)) == 1.0\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_ds = M5DataSet(self.x, self.y, self.hparams.src_len, self.hparams.tgt_len, self.hparams.bsz,dstype='train')  \n",
    "        print(f'train_ds.length - {len(train_ds)}')\n",
    "        train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, pin_memory=True)\n",
    "        return train_dl\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        test_ds = M5DataSet(self.x, self.y, self.hparams.src_len, self.hparams.tgt_len, self.hparams.bsz, dstype='test1')  \n",
    "        test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "        return test_dl\n",
    "    \n",
    "    def emb_lookups(self, xb, yb=None):\n",
    "        embs_t = []\n",
    "        for idx in range(self.hparams.num_cat_cols):\n",
    "#             print('looking up for ', idx)\n",
    "            embs_t.append(self.embs[idx](xb[:,:,idx].type(torch.long)))\n",
    "        xb_cat = torch.cat(embs_t, dim=2)\n",
    "        xb_cont = xb[:,:,self.hparams.num_cat_cols:]\n",
    "        \n",
    "        if yb is not None:\n",
    "            xb = torch.cat([xb_cat, xb_cont.type(xb_cat.dtype), yb.unsqueeze(2).type(xb_cat.dtype)], dim=2)\n",
    "        else:\n",
    "            xb = torch.cat([xb_cat, xb_cont.type(xb_cat.dtype)], dim=2)\n",
    "            \n",
    "        #pad to adjust the feature dimension\n",
    "        dim3_shortfall = self.hparams.ninp - xb.size(2)\n",
    "        assert dim3_shortfall >= 0\n",
    "        pad = nn.ConstantPad1d(padding=(0,dim3_shortfall),value=0)\n",
    "        xb = pad(xb) \n",
    "\n",
    "        return xb\n",
    "\n",
    "    def forward(self, x_src, y_src, x_tgt):        \n",
    "        x_src = self.emb_lookups(x_src, y_src)\n",
    "        x_tgt = self.emb_lookups(x_tgt)\n",
    "            \n",
    "        x_src = self.pos_encoder(x_src)\n",
    "#         print('shape after pos encoder - ', x_src.size())\n",
    "        out = self.transformer(x_src, x_tgt)\n",
    "#         print('shape after transformer - ', out.size())\n",
    "        out = self.relu(self.lin1(out))\n",
    "        out = self.sigmoid(self.lin2(out))\n",
    "        \n",
    "        return out\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        x_src, x_tgt, y_src, y_tgt, item_store_mask = batch\n",
    "        x_src = x_src.squeeze(0)\n",
    "        x_tgt = x_tgt.squeeze(0)\n",
    "        y_src = y_src.squeeze(0)\n",
    "        y_tgt = y_tgt.squeeze(0)\n",
    "        \n",
    "#         print(f'x_src.shape - {tuple(x_src.shape)} \\t x_tgt.shape - {tuple(x_tgt.shape)} \\t y_src.shape - {y_src.shape} \\t y_tgt.shape - {y_tgt.shape}')\n",
    "        yhat_tgt = self(x_src, y_src, x_tgt)\n",
    "\n",
    "        # apply the mask (due to random selection of item_stores) to output\n",
    "#         idxs = list(np.arange(0,x_src.size(1)))\n",
    "#         idxs = [1 if o in item_store_mask else 0 for o in idxs]\n",
    "#         mask = torch.tensor(idxs) * torch.ones(y_tgt.size(0), y_tgt.size(1))\n",
    "#         print(f'mask.shape: {mask.shape}')\n",
    "#         \n",
    "        loss = self.criterion((yhat_tgt).reshape(-1).type(torch.float32), (y_tgt).reshape(-1).type(torch.float32))\n",
    "        if batch_idx%10 == 0:\n",
    "            print(f'{batch_idx} loss: {loss}  yhat_tgt.sum: {yhat_tgt.sum().item()}  y_tgt.sum: {y_tgt.sum().item()}')\n",
    "            \n",
    "        return {'loss': loss}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "    \n",
    "#     def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx,\n",
    "#                        second_order_closure=None):\n",
    "#         optimizer.step()\n",
    "#         if batch_idx == 5:\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if param.requires_grad:\n",
    "#                     pass\n",
    "# #                     print(name, param.grad)\n",
    "#         optimizer.zero_grad()\n",
    "    \n",
    "    def test(self):\n",
    "        dl = self.test_dataloader()\n",
    "        batch = next(iter(dl))\n",
    "        return batch\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "   | Name                                     | Type                    | Params\n",
      "---------------------------------------------------------------------------------\n",
      "0  | pos_encoder                              | PositionalEncoding      | 0     \n",
      "1  | pos_encoder.dropout                      | Dropout                 | 0     \n",
      "2  | encoder                                  | TransformerEncoder      | 740 K \n",
      "3  | encoder.layers                           | ModuleList              | 740 K \n",
      "4  | encoder.layers.0                         | TransformerEncoderLayer | 740 K \n",
      "5  | encoder.layers.0.self_attn               | MultiheadAttention      | 410 K \n",
      "6  | encoder.layers.0.self_attn.out_proj      | Linear                  | 102 K \n",
      "7  | encoder.layers.0.linear1                 | Linear                  | 164 K \n",
      "8  | encoder.layers.0.dropout                 | Dropout                 | 0     \n",
      "9  | encoder.layers.0.linear2                 | Linear                  | 164 K \n",
      "10 | encoder.layers.0.norm1                   | LayerNorm               | 640   \n",
      "11 | encoder.layers.0.norm2                   | LayerNorm               | 640   \n",
      "12 | encoder.layers.0.dropout1                | Dropout                 | 0     \n",
      "13 | encoder.layers.0.dropout2                | Dropout                 | 0     \n",
      "14 | decoder                                  | TransformerDecoder      | 1 M   \n",
      "15 | decoder.layers                           | ModuleList              | 1 M   \n",
      "16 | decoder.layers.0                         | TransformerDecoderLayer | 1 M   \n",
      "17 | decoder.layers.0.self_attn               | MultiheadAttention      | 410 K \n",
      "18 | decoder.layers.0.self_attn.out_proj      | Linear                  | 102 K \n",
      "19 | decoder.layers.0.multihead_attn          | MultiheadAttention      | 410 K \n",
      "20 | decoder.layers.0.multihead_attn.out_proj | Linear                  | 102 K \n",
      "21 | decoder.layers.0.linear1                 | Linear                  | 164 K \n",
      "22 | decoder.layers.0.dropout                 | Dropout                 | 0     \n",
      "23 | decoder.layers.0.linear2                 | Linear                  | 164 K \n",
      "24 | decoder.layers.0.norm1                   | LayerNorm               | 640   \n",
      "25 | decoder.layers.0.norm2                   | LayerNorm               | 640   \n",
      "26 | decoder.layers.0.norm3                   | LayerNorm               | 640   \n",
      "27 | decoder.layers.0.dropout1                | Dropout                 | 0     \n",
      "28 | decoder.layers.0.dropout2                | Dropout                 | 0     \n",
      "29 | decoder.layers.0.dropout3                | Dropout                 | 0     \n",
      "30 | transformer                              | Transformer             | 1 M   \n",
      "31 | criterion                                | MSELoss                 | 0     \n",
      "32 | lin1                                     | Linear                  | 16 K  \n",
      "33 | lin2                                     | Linear                  | 51    \n",
      "34 | sigmoid                                  | Sigmoid                 | 0     \n",
      "35 | relu                                     | ReLU                    | 0     \n",
      "36 | embs                                     | ModuleList              | 436 K \n",
      "37 | embs.0                                   | Embedding               | 436 K \n",
      "38 | embs.1                                   | Embedding               | 35    \n",
      "39 | embs.2                                   | Embedding               | 9     \n",
      "40 | embs.3                                   | Embedding               | 60    \n",
      "41 | embs.4                                   | Embedding               | 9     \n",
      "42 | embs.5                                   | Embedding               | 35    \n",
      "43 | embs.6                                   | Embedding               | 35    \n",
      "44 | embs.7                                   | Embedding               | 72    \n",
      "45 | embs.8                                   | Embedding               | 24    \n",
      "46 | embs.9                                   | Embedding               | 341   \n",
      "47 | embs.10                                  | Embedding               | 20    \n",
      "48 | embs.11                                  | Embedding               | 20    \n",
      "49 | embs.12                                  | Embedding               | 9     \n",
      "50 | embs.13                                  | Embedding               | 4     \n",
      "51 | embs.14                                  | Embedding               | 4     \n",
      "52 | embs.15                                  | Embedding               | 4     \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_szs - [(3049, 143), (7, 5), (3, 3), (10, 6), (3, 3), (7, 5), (7, 5), (12, 6), (6, 4), (31, 11), (5, 4), (5, 4), (3, 3), (2, 2), (2, 2), (2, 2)]\n",
      "train_ds.length - 1795\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65f5e2c0e884441a9bf4d59502ba4c2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', description='Training', layout=Layout(flex='2'), max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss: 0.20012733340263367  yhat_tgt.sum: 12483.1416015625  y_tgt.sum: 41.56880733944954\n",
      "10 loss: 0.010644232854247093  yhat_tgt.sum: 2876.50927734375  y_tgt.sum: 44.46002621231979\n",
      "20 loss: 0.0016893543070182204  yhat_tgt.sum: 1165.6453857421875  y_tgt.sum: 40.94888597640892\n",
      "30 loss: 0.000650092726573348  yhat_tgt.sum: 730.61328125  y_tgt.sum: 40.542595019659245\n",
      "40 loss: 0.00037540795165114105  yhat_tgt.sum: 562.69287109375  y_tgt.sum: 49.76277850589777\n",
      "50 loss: 0.0003016259288415313  yhat_tgt.sum: 508.742431640625  y_tgt.sum: 42.84534731323722\n",
      "60 loss: 0.0002538607513997704  yhat_tgt.sum: 470.6900634765625  y_tgt.sum: 48.72739187418087\n",
      "70 loss: 0.00025422172620892525  yhat_tgt.sum: 443.327392578125  y_tgt.sum: 40.23722149410223\n",
      "80 loss: 0.00021273642778396606  yhat_tgt.sum: 424.31536865234375  y_tgt.sum: 37.418086500655306\n",
      "90 loss: 0.00019020464969798923  yhat_tgt.sum: 393.92236328125  y_tgt.sum: 41.972477064220186\n",
      "100 loss: 0.00016789651999715716  yhat_tgt.sum: 384.636474609375  y_tgt.sum: 49.530799475753604\n",
      "110 loss: 0.0001651683123782277  yhat_tgt.sum: 368.135009765625  y_tgt.sum: 39.18348623853211\n",
      "120 loss: 0.00016659346874803305  yhat_tgt.sum: 366.1878356933594  y_tgt.sum: 33.13499344692005\n",
      "130 loss: 0.00014336600725073367  yhat_tgt.sum: 334.97607421875  y_tgt.sum: 42.99213630406291\n",
      "140 loss: 0.00012607721146196127  yhat_tgt.sum: 318.65081787109375  y_tgt.sum: 40.89777195281783\n",
      "150 loss: 0.00011570470815058798  yhat_tgt.sum: 316.49859619140625  y_tgt.sum: 39.72739187418087\n",
      "160 loss: 0.00012088634684914723  yhat_tgt.sum: 305.0774230957031  y_tgt.sum: 48.65268676277851\n",
      "170 loss: 9.967756341211498e-05  yhat_tgt.sum: 292.06097412109375  y_tgt.sum: 45.37483617300131\n",
      "180 loss: 0.00010371673124609515  yhat_tgt.sum: 289.4023742675781  y_tgt.sum: 45.25950196592399\n",
      "190 loss: 9.327471343567595e-05  yhat_tgt.sum: 275.6329345703125  y_tgt.sum: 30.7562254259502\n",
      "200 loss: 9.541720646666363e-05  yhat_tgt.sum: 275.5099792480469  y_tgt.sum: 39.87942332896462\n",
      "210 loss: 7.572869799332693e-05  yhat_tgt.sum: 259.2370300292969  y_tgt.sum: 40.72346002621232\n",
      "220 loss: 8.332226570928469e-05  yhat_tgt.sum: 252.3631134033203  y_tgt.sum: 46.24246395806029\n",
      "230 loss: 6.703088729409501e-05  yhat_tgt.sum: 240.78485107421875  y_tgt.sum: 39.97771952817824\n",
      "240 loss: 6.911248055985197e-05  yhat_tgt.sum: 244.20132446289062  y_tgt.sum: 43.82437745740498\n",
      "250 loss: 7.344278128584847e-05  yhat_tgt.sum: 233.5331268310547  y_tgt.sum: 43.60812581913499\n",
      "260 loss: 7.110180740710348e-05  yhat_tgt.sum: 226.24444580078125  y_tgt.sum: 42.51769331585845\n",
      "270 loss: 7.028501568129286e-05  yhat_tgt.sum: 221.431884765625  y_tgt.sum: 48.74442988204456\n",
      "280 loss: 5.511800554813817e-05  yhat_tgt.sum: 216.9512939453125  y_tgt.sum: 43.47313237221494\n",
      "290 loss: 5.1815459300996736e-05  yhat_tgt.sum: 210.55862426757812  y_tgt.sum: 45.78374836173002\n",
      "300 loss: 5.3440893680090085e-05  yhat_tgt.sum: 204.5891876220703  y_tgt.sum: 42.22018348623853\n",
      "310 loss: 5.0758411816786975e-05  yhat_tgt.sum: 203.5910186767578  y_tgt.sum: 32.587155963302756\n",
      "320 loss: 5.1404389523668215e-05  yhat_tgt.sum: 200.14352416992188  y_tgt.sum: 42.38138925294889\n",
      "330 loss: 5.307998071657494e-05  yhat_tgt.sum: 193.47679138183594  y_tgt.sum: 43.16120576671035\n",
      "340 loss: 4.905671812593937e-05  yhat_tgt.sum: 192.22079467773438  y_tgt.sum: 26.910878112712975\n",
      "350 loss: 4.562433241517283e-05  yhat_tgt.sum: 190.16793823242188  y_tgt.sum: 27.85058977719528\n",
      "360 loss: 4.2256790038663894e-05  yhat_tgt.sum: 182.81573486328125  y_tgt.sum: 45.144167758846656\n",
      "370 loss: 3.913603359251283e-05  yhat_tgt.sum: 176.41513061523438  y_tgt.sum: 42.534731323722156\n",
      "380 loss: 6.366957677528262e-05  yhat_tgt.sum: 179.12802124023438  y_tgt.sum: 49.91087811271298\n",
      "390 loss: 4.266254836693406e-05  yhat_tgt.sum: 178.97589111328125  y_tgt.sum: 49.07077326343382\n",
      "400 loss: 4.67208337795455e-05  yhat_tgt.sum: 175.3584747314453  y_tgt.sum: 49.71821756225426\n",
      "410 loss: 3.672846651170403e-05  yhat_tgt.sum: 167.9765625  y_tgt.sum: 39.73656618610747\n",
      "420 loss: 3.885940168402158e-05  yhat_tgt.sum: 166.42767333984375  y_tgt.sum: 41.83224115334207\n",
      "430 loss: 8.590598008595407e-05  yhat_tgt.sum: 160.31231689453125  y_tgt.sum: 40.51769331585845\n",
      "440 loss: 3.8081194361438975e-05  yhat_tgt.sum: 165.64498901367188  y_tgt.sum: 39.41022280471822\n",
      "450 loss: 5.7662120525492355e-05  yhat_tgt.sum: 159.46347045898438  y_tgt.sum: 36.980340760157276\n",
      "460 loss: 5.067708480055444e-05  yhat_tgt.sum: 158.15850830078125  y_tgt.sum: 44.04456094364352\n",
      "470 loss: 3.70913403457962e-05  yhat_tgt.sum: 154.83592224121094  y_tgt.sum: 41.46657929226737\n",
      "480 loss: 4.1863917431328446e-05  yhat_tgt.sum: 156.78323364257812  y_tgt.sum: 41.73263433813892\n",
      "490 loss: 3.931940591428429e-05  yhat_tgt.sum: 149.00009155273438  y_tgt.sum: 44.48099606815204\n",
      "500 loss: 3.8598478568019345e-05  yhat_tgt.sum: 144.4508056640625  y_tgt.sum: 41.005242463958055\n",
      "510 loss: 6.022966772434302e-05  yhat_tgt.sum: 146.30453491210938  y_tgt.sum: 51.795543905635654\n",
      "520 loss: 2.8181777452118695e-05  yhat_tgt.sum: 148.6822052001953  y_tgt.sum: 38.698558322411536\n",
      "530 loss: 3.58239485649392e-05  yhat_tgt.sum: 143.86724853515625  y_tgt.sum: 33.773263433813895\n",
      "540 loss: 3.935487984563224e-05  yhat_tgt.sum: 141.7225799560547  y_tgt.sum: 49.446920052424645\n",
      "550 loss: 4.0488666854798794e-05  yhat_tgt.sum: 136.31927490234375  y_tgt.sum: 40.37221494102228\n",
      "560 loss: 2.866938302759081e-05  yhat_tgt.sum: 138.16636657714844  y_tgt.sum: 40.34076015727392\n",
      "570 loss: 3.751916665351018e-05  yhat_tgt.sum: 137.75045776367188  y_tgt.sum: 48.4875491480996\n",
      "580 loss: 2.8280957849347033e-05  yhat_tgt.sum: 132.71112060546875  y_tgt.sum: 43.250327653997374\n",
      "590 loss: 2.7556956410990097e-05  yhat_tgt.sum: 129.67953491210938  y_tgt.sum: 40.60288335517693\n",
      "600 loss: 2.7159854653291404e-05  yhat_tgt.sum: 130.99212646484375  y_tgt.sum: 47.49541284403669\n",
      "610 loss: 3.069606827921234e-05  yhat_tgt.sum: 126.7742919921875  y_tgt.sum: 42.055045871559635\n",
      "620 loss: 2.8958054826944135e-05  yhat_tgt.sum: 127.5539321899414  y_tgt.sum: 40.84927916120577\n",
      "630 loss: 4.196755253360607e-05  yhat_tgt.sum: 127.74064636230469  y_tgt.sum: 37.46264744429882\n",
      "640 loss: 5.395673724706285e-05  yhat_tgt.sum: 128.6978759765625  y_tgt.sum: 39.956749672346\n",
      "650 loss: 3.425926115596667e-05  yhat_tgt.sum: 123.99760437011719  y_tgt.sum: 33.28440366972477\n",
      "660 loss: 5.3816325817024335e-05  yhat_tgt.sum: 122.41669464111328  y_tgt.sum: 55.07339449541284\n",
      "670 loss: 4.638932296074927e-05  yhat_tgt.sum: 121.59573364257812  y_tgt.sum: 39.655307994757536\n",
      "680 loss: 4.921050640405156e-05  yhat_tgt.sum: 119.63957214355469  y_tgt.sum: 47.20576671035387\n",
      "690 loss: 2.8784534151782282e-05  yhat_tgt.sum: 118.1604232788086  y_tgt.sum: 39.36828309305373\n",
      "700 loss: 2.2931486455490813e-05  yhat_tgt.sum: 116.23915100097656  y_tgt.sum: 37.91874180865007\n",
      "710 loss: 1.839548713178374e-05  yhat_tgt.sum: 115.77232360839844  y_tgt.sum: 44.59895150720838\n",
      "720 loss: 4.325472764321603e-05  yhat_tgt.sum: 116.02888488769531  y_tgt.sum: 45.646133682830936\n",
      "730 loss: 5.671190228895284e-05  yhat_tgt.sum: 113.43522644042969  y_tgt.sum: 49.49410222804718\n",
      "740 loss: 2.975476672872901e-05  yhat_tgt.sum: 114.79719543457031  y_tgt.sum: 39.32110091743119\n",
      "750 loss: 2.412171488685999e-05  yhat_tgt.sum: 115.16145324707031  y_tgt.sum: 40.61467889908257\n",
      "760 loss: 4.3440304580144584e-05  yhat_tgt.sum: 111.60395812988281  y_tgt.sum: 46.647444298820446\n",
      "770 loss: 1.990737291635014e-05  yhat_tgt.sum: 109.34391784667969  y_tgt.sum: 34.416775884665796\n",
      "780 loss: 3.418701089685783e-05  yhat_tgt.sum: 109.96573638916016  y_tgt.sum: 36.57798165137615\n",
      "790 loss: 2.653590854606591e-05  yhat_tgt.sum: 107.90248107910156  y_tgt.sum: 47.80078636959371\n",
      "800 loss: 4.3190273572690785e-05  yhat_tgt.sum: 107.89566040039062  y_tgt.sum: 40.63302752293578\n",
      "810 loss: 3.926605495507829e-05  yhat_tgt.sum: 106.65060424804688  y_tgt.sum: 49.277850589777195\n",
      "820 loss: 2.55332633969374e-05  yhat_tgt.sum: 106.28791809082031  y_tgt.sum: 34.60681520314548\n",
      "830 loss: 2.4543209292460233e-05  yhat_tgt.sum: 107.4000015258789  y_tgt.sum: 41.753604193971164\n",
      "840 loss: 3.0195171348168515e-05  yhat_tgt.sum: 104.55380249023438  y_tgt.sum: 46.551769331585845\n",
      "850 loss: 2.525442323531024e-05  yhat_tgt.sum: 103.14213562011719  y_tgt.sum: 38.862385321100916\n",
      "860 loss: 2.4059276256593876e-05  yhat_tgt.sum: 101.47135925292969  y_tgt.sum: 42.89777195281782\n",
      "870 loss: 2.944316111097578e-05  yhat_tgt.sum: 101.06810760498047  y_tgt.sum: 42.01310615989515\n",
      "880 loss: 2.637388934090268e-05  yhat_tgt.sum: 101.41714477539062  y_tgt.sum: 37.71035386631717\n",
      "890 loss: 4.9308291636407375e-05  yhat_tgt.sum: 100.01568603515625  y_tgt.sum: 51.1559633027523\n",
      "900 loss: 3.1695544748799875e-05  yhat_tgt.sum: 99.78399658203125  y_tgt.sum: 34.83617300131061\n",
      "910 loss: 2.1776211724500172e-05  yhat_tgt.sum: 98.14913940429688  y_tgt.sum: 38.49672346002622\n",
      "920 loss: 3.16958648909349e-05  yhat_tgt.sum: 98.12541198730469  y_tgt.sum: 45.22018348623853\n",
      "930 loss: 1.4137640391709283e-05  yhat_tgt.sum: 97.74269104003906  y_tgt.sum: 34.38663171690695\n",
      "940 loss: 4.0517705201637e-05  yhat_tgt.sum: 95.3051986694336  y_tgt.sum: 46.29619921363041\n",
      "950 loss: 2.2528509362018667e-05  yhat_tgt.sum: 94.71807861328125  y_tgt.sum: 38.17562254259502\n",
      "960 loss: 1.4166593246045522e-05  yhat_tgt.sum: 96.70781707763672  y_tgt.sum: 28.840104849279165\n",
      "970 loss: 2.559675158408936e-05  yhat_tgt.sum: 96.21452331542969  y_tgt.sum: 43.78636959370905\n",
      "980 loss: 2.553258127591107e-05  yhat_tgt.sum: 93.48846435546875  y_tgt.sum: 38.0956749672346\n",
      "990 loss: 2.4670333004905842e-05  yhat_tgt.sum: 93.50543212890625  y_tgt.sum: 48.49672346002622\n",
      "1000 loss: 2.104071609210223e-05  yhat_tgt.sum: 92.54484558105469  y_tgt.sum: 44.66448230668414\n",
      "1010 loss: 0.00010130011651199311  yhat_tgt.sum: 91.99472045898438  y_tgt.sum: 54.60681520314548\n",
      "1020 loss: 1.8683740563574247e-05  yhat_tgt.sum: 91.46745300292969  y_tgt.sum: 40.81913499344692\n",
      "1030 loss: 2.6664440156309865e-05  yhat_tgt.sum: 89.3946533203125  y_tgt.sum: 44.070773263433814\n",
      "1040 loss: 2.2240757971303537e-05  yhat_tgt.sum: 88.69284057617188  y_tgt.sum: 41.72346002621232\n",
      "1050 loss: 3.9512051444035023e-05  yhat_tgt.sum: 89.16827392578125  y_tgt.sum: 48.68152031454784\n",
      "1060 loss: 2.7512780434335582e-05  yhat_tgt.sum: 90.29114532470703  y_tgt.sum: 32.36697247706422\n",
      "1070 loss: 1.95775501197204e-05  yhat_tgt.sum: 88.85592651367188  y_tgt.sum: 44.9305373525557\n",
      "1080 loss: 2.7524432880454697e-05  yhat_tgt.sum: 87.39566040039062  y_tgt.sum: 41.750982961992136\n",
      "1090 loss: 4.2110532376682386e-05  yhat_tgt.sum: 86.5353012084961  y_tgt.sum: 48.424639580602886\n",
      "1100 loss: 2.2666377844871022e-05  yhat_tgt.sum: 86.67291259765625  y_tgt.sum: 29.897771952817823\n",
      "1110 loss: 4.456634633243084e-05  yhat_tgt.sum: 85.62377166748047  y_tgt.sum: 47.59501965923984\n",
      "1120 loss: 2.141068034688942e-05  yhat_tgt.sum: 85.9769515991211  y_tgt.sum: 42.31585845347313\n",
      "1130 loss: 2.3401746148010716e-05  yhat_tgt.sum: 84.03788757324219  y_tgt.sum: 36.416775884665796\n",
      "1140 loss: 1.9864544810843654e-05  yhat_tgt.sum: 84.7095718383789  y_tgt.sum: 45.89121887287025\n",
      "1150 loss: 1.938460991368629e-05  yhat_tgt.sum: 84.19338989257812  y_tgt.sum: 44.7038007863696\n",
      "1160 loss: 2.440666503389366e-05  yhat_tgt.sum: 83.58724975585938  y_tgt.sum: 41.25688073394495\n",
      "1170 loss: 1.7032598407240584e-05  yhat_tgt.sum: 82.41802215576172  y_tgt.sum: 41.94233289646134\n",
      "1180 loss: 3.272359390393831e-05  yhat_tgt.sum: 81.586669921875  y_tgt.sum: 49.47444298820446\n",
      "1190 loss: 2.400570156169124e-05  yhat_tgt.sum: 83.55780029296875  y_tgt.sum: 37.901703800786365\n",
      "1200 loss: 2.9586703021777794e-05  yhat_tgt.sum: 81.31036376953125  y_tgt.sum: 39.90563564875491\n",
      "1210 loss: 4.0072914998745546e-05  yhat_tgt.sum: 80.30229187011719  y_tgt.sum: 46.03276539973788\n",
      "1220 loss: 2.234376734122634e-05  yhat_tgt.sum: 79.99169158935547  y_tgt.sum: 45.766710353866316\n",
      "1230 loss: 1.3824514098814689e-05  yhat_tgt.sum: 80.61710357666016  y_tgt.sum: 39.967234600262124\n",
      "1240 loss: 4.0114318835549057e-05  yhat_tgt.sum: 79.88558197021484  y_tgt.sum: 42.77457404980341\n",
      "1250 loss: 1.6696078091626987e-05  yhat_tgt.sum: 79.00032043457031  y_tgt.sum: 29.175622542595022\n",
      "1260 loss: 1.7066795408027247e-05  yhat_tgt.sum: 78.52528381347656  y_tgt.sum: 40.18479685452162\n",
      "1270 loss: 3.352725980221294e-05  yhat_tgt.sum: 78.56703186035156  y_tgt.sum: 48.17955439056357\n",
      "1280 loss: 4.571461249724962e-05  yhat_tgt.sum: 77.48876953125  y_tgt.sum: 52.84665792922674\n",
      "1290 loss: 5.1134058594470844e-05  yhat_tgt.sum: 76.99209594726562  y_tgt.sum: 39.00524246395806\n",
      "1300 loss: 2.4155129722203128e-05  yhat_tgt.sum: 76.5591812133789  y_tgt.sum: 42.138925294888594\n",
      "1310 loss: 2.1179384930292144e-05  yhat_tgt.sum: 76.42603302001953  y_tgt.sum: 35.00786369593709\n",
      "1320 loss: 2.391671296209097e-05  yhat_tgt.sum: 75.27867126464844  y_tgt.sum: 46.0170380078637\n",
      "1330 loss: 2.1342322725104168e-05  yhat_tgt.sum: 75.81446075439453  y_tgt.sum: 40.373525557011796\n",
      "1340 loss: 2.436787326587364e-05  yhat_tgt.sum: 74.77021789550781  y_tgt.sum: 36.85583224115334\n",
      "1350 loss: 1.7926447981153615e-05  yhat_tgt.sum: 75.50698852539062  y_tgt.sum: 30.301441677588468\n",
      "1360 loss: 3.4194756153738126e-05  yhat_tgt.sum: 75.70072174072266  y_tgt.sum: 39.859764089121896\n",
      "1370 loss: 1.5215825442282949e-05  yhat_tgt.sum: 74.54548645019531  y_tgt.sum: 32.4613368283093\n",
      "1380 loss: 2.2954838641453534e-05  yhat_tgt.sum: 76.65583801269531  y_tgt.sum: 42.82306684141547\n",
      "1390 loss: 4.8872701881919056e-05  yhat_tgt.sum: 73.38617706298828  y_tgt.sum: 47.537352555701176\n",
      "1400 loss: 1.6473306459374726e-05  yhat_tgt.sum: 74.01097869873047  y_tgt.sum: 26.980340760157276\n",
      "1410 loss: 5.2883078751619905e-05  yhat_tgt.sum: 72.52993774414062  y_tgt.sum: 55.56880733944954\n",
      "1420 loss: 2.3853839593357407e-05  yhat_tgt.sum: 73.14398193359375  y_tgt.sum: 49.27260812581913\n",
      "1430 loss: 3.703712354763411e-05  yhat_tgt.sum: 72.90178680419922  y_tgt.sum: 47.07470511140236\n",
      "1440 loss: 5.30302531842608e-05  yhat_tgt.sum: 71.6712646484375  y_tgt.sum: 49.95412844036697\n",
      "1450 loss: 3.2680967706255615e-05  yhat_tgt.sum: 71.44432067871094  y_tgt.sum: 44.595019659239846\n",
      "1460 loss: 2.4089427824947052e-05  yhat_tgt.sum: 71.36243438720703  y_tgt.sum: 43.29882044560944\n",
      "1470 loss: 2.2502621504827403e-05  yhat_tgt.sum: 70.53955841064453  y_tgt.sum: 39.38532110091744\n",
      "1480 loss: 1.3285638488014229e-05  yhat_tgt.sum: 68.99050903320312  y_tgt.sum: 40.749672346002626\n",
      "1490 loss: 2.203333133365959e-05  yhat_tgt.sum: 70.049560546875  y_tgt.sum: 50.306684141546526\n",
      "1500 loss: 1.5127316146390513e-05  yhat_tgt.sum: 70.8503646850586  y_tgt.sum: 38.17169069462648\n",
      "1510 loss: 3.7701531255152076e-05  yhat_tgt.sum: 70.45989990234375  y_tgt.sum: 49.646133682830936\n",
      "1520 loss: 2.728431900322903e-05  yhat_tgt.sum: 69.75180053710938  y_tgt.sum: 30.80602883355177\n",
      "1530 loss: 2.028828203037847e-05  yhat_tgt.sum: 68.92778015136719  y_tgt.sum: 46.73656618610747\n",
      "1540 loss: 2.1909683709964156e-05  yhat_tgt.sum: 69.31404113769531  y_tgt.sum: 38.95412844036697\n",
      "1550 loss: 1.8433189325151034e-05  yhat_tgt.sum: 68.12239074707031  y_tgt.sum: 34.21625163826999\n",
      "1560 loss: 6.228363054106012e-05  yhat_tgt.sum: 67.6130599975586  y_tgt.sum: 47.14154652686763\n",
      "1570 loss: 2.9464397812262177e-05  yhat_tgt.sum: 69.73390197753906  y_tgt.sum: 39.60812581913499\n",
      "1580 loss: 3.203614323865622e-05  yhat_tgt.sum: 66.71563720703125  y_tgt.sum: 44.795543905635654\n",
      "1590 loss: 1.105818500946043e-05  yhat_tgt.sum: 67.34555053710938  y_tgt.sum: 26.404980340760158\n",
      "1600 loss: 2.4112590836011805e-05  yhat_tgt.sum: 67.51922607421875  y_tgt.sum: 41.77981651376147\n",
      "1610 loss: 2.144927202607505e-05  yhat_tgt.sum: 66.56983184814453  y_tgt.sum: 48.33158584534731\n",
      "1620 loss: 1.8402230125502683e-05  yhat_tgt.sum: 66.89549255371094  y_tgt.sum: 30.157273918741808\n",
      "1630 loss: 2.1356181605369784e-05  yhat_tgt.sum: 67.71007537841797  y_tgt.sum: 41.42988204456094\n",
      "1640 loss: 3.2326686778105795e-05  yhat_tgt.sum: 66.09336853027344  y_tgt.sum: 46.89777195281783\n",
      "1650 loss: 1.3574927834270056e-05  yhat_tgt.sum: 65.86375427246094  y_tgt.sum: 44.60550458715596\n",
      "1660 loss: 1.5971816537785344e-05  yhat_tgt.sum: 64.75106811523438  y_tgt.sum: 36.04456094364351\n",
      "1670 loss: 3.335211658850312e-05  yhat_tgt.sum: 65.11871337890625  y_tgt.sum: 47.10222804718218\n",
      "1680 loss: 4.675820309785195e-05  yhat_tgt.sum: 65.0488052368164  y_tgt.sum: 49.57667103538664\n",
      "1690 loss: 1.859891381172929e-05  yhat_tgt.sum: 65.08692932128906  y_tgt.sum: 30.14154652686763\n",
      "1700 loss: 2.941622915386688e-05  yhat_tgt.sum: 63.47320556640625  y_tgt.sum: 51.322411533420706\n",
      "1710 loss: 1.6362360838684253e-05  yhat_tgt.sum: 64.48576354980469  y_tgt.sum: 41.04587155963303\n",
      "1720 loss: 1.6528811102034524e-05  yhat_tgt.sum: 64.16796112060547  y_tgt.sum: 44.00786369593709\n",
      "1730 loss: 1.5154355423874222e-05  yhat_tgt.sum: 65.42929077148438  y_tgt.sum: 34.29357798165138\n",
      "1740 loss: 1.2651155884668697e-05  yhat_tgt.sum: 64.98158264160156  y_tgt.sum: 34.853211009174316\n",
      "1750 loss: 1.5278024875442497e-05  yhat_tgt.sum: 63.51403045654297  y_tgt.sum: 42.03145478374836\n",
      "1760 loss: 2.223643423349131e-05  yhat_tgt.sum: 62.9539909362793  y_tgt.sum: 44.8519003931848\n",
      "1770 loss: 2.8314816518104635e-05  yhat_tgt.sum: 62.514923095703125  y_tgt.sum: 50.8440366972477\n",
      "1780 loss: 3.030307561857626e-05  yhat_tgt.sum: 64.61711883544922  y_tgt.sum: 33.42857142857143\n",
      "1790 loss: 3.572220521164127e-05  yhat_tgt.sum: 61.358604431152344  y_tgt.sum: 39.49279161205767\n",
      "\n"
     ]
    }
   ],
   "source": [
    "src_len = 28\n",
    "tgt_len = 28\n",
    "# bsz = 200\n",
    "# model = SalesModel(hparams)\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser = SalesModel.add_model_specifi_args(parser)\n",
    "hparams = parser.parse_args('--bsz 1000 --ninp 320 --nhid 512 --nlayers 1'.split())\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='models/weights.ckpt',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model = SalesModel(hparams)\n",
    "\n",
    "wandb_logger = WandbLogger(name='achinta',project='kaggle-m5-forecasting-accuracy')\n",
    "trainer = Trainer(gpus=1,max_epochs=1)\n",
    "trainer.fit(model)\n",
    "trainer.save_checkpoint('models/weights.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n",
      "emb_szs - [(3049, 143), (7, 5), (3, 3), (10, 6), (3, 3), (7, 5), (7, 5), (12, 6), (6, 4), (31, 11), (5, 4), (5, 4), (3, 3), (2, 2), (2, 2), (2, 2)]\n",
      "test1 index -  1823\n",
      "x_src.shape - torch.Size([90, 30490, 17]), x_tgt.shape - torch.Size([28, 30490, 17]) , y_src.shape - torch.Size([90, 30490]) , y_tgt.shape - torch.Size([28, 30490])\n",
      "starting inference...\n",
      "CPU times: user 7min 57s, sys: 5min 59s, total: 13min 57s\n",
      "Wall time: 29.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([28, 30490, 1])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "model = SalesModel.load_from_checkpoint('models/weights.ckpt')\n",
    "x_src, x_tgt, y_src, y_tgt, item_store_mask = model.test()\n",
    "x_src = x_src.squeeze(0)\n",
    "x_tgt = x_tgt.squeeze(0)\n",
    "y_src = y_src.squeeze(0)\n",
    "y_tgt = y_tgt.squeeze(0)\n",
    "print(f'x_src.shape - {x_src.shape}, x_tgt.shape - {x_tgt.shape} , y_src.shape - {y_src.shape} , y_tgt.shape - {y_tgt.shape}')\n",
    "\n",
    "model.eval()\n",
    "print('starting inference...')\n",
    "yhat_tgt = model(x_src, y_src, x_tgt)\n",
    "yhat_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yhat.shape:  (30490, 28)\n",
      "(60980, 29)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>F1</th>\n",
       "      <th>F2</th>\n",
       "      <th>F3</th>\n",
       "      <th>F4</th>\n",
       "      <th>F5</th>\n",
       "      <th>F6</th>\n",
       "      <th>F7</th>\n",
       "      <th>F8</th>\n",
       "      <th>F9</th>\n",
       "      <th>...</th>\n",
       "      <th>F19</th>\n",
       "      <th>F20</th>\n",
       "      <th>F21</th>\n",
       "      <th>F22</th>\n",
       "      <th>F23</th>\n",
       "      <th>F24</th>\n",
       "      <th>F25</th>\n",
       "      <th>F26</th>\n",
       "      <th>F27</th>\n",
       "      <th>F28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>0.839935</td>\n",
       "      <td>0.837773</td>\n",
       "      <td>0.863902</td>\n",
       "      <td>0.870157</td>\n",
       "      <td>0.871777</td>\n",
       "      <td>0.876341</td>\n",
       "      <td>0.840973</td>\n",
       "      <td>0.821003</td>\n",
       "      <td>0.833808</td>\n",
       "      <td>...</td>\n",
       "      <td>0.857280</td>\n",
       "      <td>0.837996</td>\n",
       "      <td>0.833920</td>\n",
       "      <td>0.815527</td>\n",
       "      <td>0.814636</td>\n",
       "      <td>0.843375</td>\n",
       "      <td>0.843909</td>\n",
       "      <td>0.848988</td>\n",
       "      <td>0.831372</td>\n",
       "      <td>0.824250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>0.804520</td>\n",
       "      <td>0.805390</td>\n",
       "      <td>0.824687</td>\n",
       "      <td>0.832469</td>\n",
       "      <td>0.832042</td>\n",
       "      <td>0.836459</td>\n",
       "      <td>0.812143</td>\n",
       "      <td>0.789161</td>\n",
       "      <td>0.807011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.829949</td>\n",
       "      <td>0.801339</td>\n",
       "      <td>0.801729</td>\n",
       "      <td>0.784564</td>\n",
       "      <td>0.785078</td>\n",
       "      <td>0.810752</td>\n",
       "      <td>0.813790</td>\n",
       "      <td>0.814745</td>\n",
       "      <td>0.794951</td>\n",
       "      <td>0.787920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>0.777685</td>\n",
       "      <td>0.780138</td>\n",
       "      <td>0.794749</td>\n",
       "      <td>0.801482</td>\n",
       "      <td>0.799718</td>\n",
       "      <td>0.810697</td>\n",
       "      <td>0.790348</td>\n",
       "      <td>0.765598</td>\n",
       "      <td>0.784383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.800609</td>\n",
       "      <td>0.773713</td>\n",
       "      <td>0.779528</td>\n",
       "      <td>0.761146</td>\n",
       "      <td>0.761870</td>\n",
       "      <td>0.781501</td>\n",
       "      <td>0.785888</td>\n",
       "      <td>0.785603</td>\n",
       "      <td>0.767900</td>\n",
       "      <td>0.760157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>0.824755</td>\n",
       "      <td>0.826054</td>\n",
       "      <td>0.843359</td>\n",
       "      <td>0.853326</td>\n",
       "      <td>0.850731</td>\n",
       "      <td>0.863657</td>\n",
       "      <td>0.825904</td>\n",
       "      <td>0.807313</td>\n",
       "      <td>0.817501</td>\n",
       "      <td>...</td>\n",
       "      <td>0.833096</td>\n",
       "      <td>0.821693</td>\n",
       "      <td>0.814602</td>\n",
       "      <td>0.800831</td>\n",
       "      <td>0.802309</td>\n",
       "      <td>0.821802</td>\n",
       "      <td>0.830176</td>\n",
       "      <td>0.830114</td>\n",
       "      <td>0.814290</td>\n",
       "      <td>0.803011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>0.800208</td>\n",
       "      <td>0.804356</td>\n",
       "      <td>0.820217</td>\n",
       "      <td>0.825491</td>\n",
       "      <td>0.822009</td>\n",
       "      <td>0.823909</td>\n",
       "      <td>0.794772</td>\n",
       "      <td>0.787144</td>\n",
       "      <td>0.796695</td>\n",
       "      <td>...</td>\n",
       "      <td>0.805924</td>\n",
       "      <td>0.792790</td>\n",
       "      <td>0.792054</td>\n",
       "      <td>0.784285</td>\n",
       "      <td>0.786825</td>\n",
       "      <td>0.805514</td>\n",
       "      <td>0.804826</td>\n",
       "      <td>0.803876</td>\n",
       "      <td>0.791274</td>\n",
       "      <td>0.786439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              id        F1        F2        F3        F4  \\\n",
       "0  HOBBIES_1_001_CA_1_validation  0.839935  0.837773  0.863902  0.870157   \n",
       "1  HOBBIES_1_002_CA_1_validation  0.804520  0.805390  0.824687  0.832469   \n",
       "2  HOBBIES_1_003_CA_1_validation  0.777685  0.780138  0.794749  0.801482   \n",
       "3  HOBBIES_1_004_CA_1_validation  0.824755  0.826054  0.843359  0.853326   \n",
       "4  HOBBIES_1_005_CA_1_validation  0.800208  0.804356  0.820217  0.825491   \n",
       "\n",
       "         F5        F6        F7        F8        F9  ...       F19       F20  \\\n",
       "0  0.871777  0.876341  0.840973  0.821003  0.833808  ...  0.857280  0.837996   \n",
       "1  0.832042  0.836459  0.812143  0.789161  0.807011  ...  0.829949  0.801339   \n",
       "2  0.799718  0.810697  0.790348  0.765598  0.784383  ...  0.800609  0.773713   \n",
       "3  0.850731  0.863657  0.825904  0.807313  0.817501  ...  0.833096  0.821693   \n",
       "4  0.822009  0.823909  0.794772  0.787144  0.796695  ...  0.805924  0.792790   \n",
       "\n",
       "        F21       F22       F23       F24       F25       F26       F27  \\\n",
       "0  0.833920  0.815527  0.814636  0.843375  0.843909  0.848988  0.831372   \n",
       "1  0.801729  0.784564  0.785078  0.810752  0.813790  0.814745  0.794951   \n",
       "2  0.779528  0.761146  0.761870  0.781501  0.785888  0.785603  0.767900   \n",
       "3  0.814602  0.800831  0.802309  0.821802  0.830176  0.830114  0.814290   \n",
       "4  0.792054  0.784285  0.786825  0.805514  0.804826  0.803876  0.791274   \n",
       "\n",
       "        F28  \n",
       "0  0.824250  \n",
       "1  0.787920  \n",
       "2  0.760157  \n",
       "3  0.803011  \n",
       "4  0.786439  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yhat_tgt = yhat_tgt.refine_names('days','item_store','demand')\n",
    "yhat_tgt_aligned = yhat_tgt.align_to('item_store','days','demand').squeeze(2).detach().numpy()\n",
    "print(f'yhat.shape: ', yhat_tgt_aligned.shape)\n",
    "\n",
    "# create preds df\n",
    "preds = pd.DataFrame()\n",
    "preds['id'] = sales['id']\n",
    "\n",
    "# read scalers\n",
    "with open('scalers.pkl','rb') as f:\n",
    "    scalers = pickle.load(f)\n",
    "\n",
    "\n",
    "pred_ids = preds['id'].tolist()\n",
    "# eval df should also be submitted (days 1942 to 1969)\n",
    "eval_ids = ['_'.join(o.split('_')[:5] + ['evaluation']) for o in pred_ids]\n",
    "eval_df = pd.DataFrame({'id': eval_ids})\n",
    "\n",
    "for idx in range(num_test1_days):\n",
    "    preds['F' + str(idx+1)] = yhat_tgt_aligned[:,idx]\n",
    "    preds['F' + str(idx+1)] = scalers['demand'].inverse_transform(preds[['F' + str(idx+1)]])\n",
    "    \n",
    "    eval_df['F' + str(idx+1)] = 0.0\n",
    "    \n",
    "out_df = pd.concat([preds,eval_df],axis=0)\n",
    "print(out_df.shape)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|##########| 20.4M/20.4M [00:09<00:00, 2.23MB/s]\n",
      "Successfully submitted to M5 Forecasting - Accuracy"
     ]
    }
   ],
   "source": [
    "out_df.to_csv('preds.csv', index=False)\n",
    "!kaggle competitions submit -c m5-forecasting-accuracy -f preds.csv -m \"transformers 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head preds.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_store_mask = list(np.random.randint(0, 10,3))\n",
    "item_store_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(10).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(hparams)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(data_dir/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
