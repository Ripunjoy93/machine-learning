{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# !pip install --upgrade pytorch_lightning wandb\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import RobustScaler, LabelEncoder, OrdinalEncoder, MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import numpy as np\n",
    "\n",
    "from pathlib import Path\n",
    "from argparse import ArgumentParser\n",
    "import math\n",
    "import wandb\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pickle\n",
    "\n",
    "\n",
    "data_dir = Path.home()/'data/kaggle/m5-forecasting-accuracy'\n",
    "\n",
    "x_cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id',\n",
    "        'weekday', 'wday', 'month', 'year',\n",
    "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "       'snap_CA', 'snap_TX', 'snap_WI']\n",
    "\n",
    "x_cont_cols = ['sell_price']\n",
    "num_train_val_days = 1913\n",
    "num_test1_days = 28\n",
    "num_test2_days = 28\n",
    "\n",
    "src_len = 28\n",
    "tgt_len = num_test1_days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TODO\n",
    " - normalize y\n",
    " - sales price is 0. fix it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales = pd.read_csv(data_dir/'sales_train_validation.csv')\n",
    "print(f'sales.shape: {sales.shape}')\n",
    "cat_cols = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']\n",
    "\n",
    "# encode cat cols\n",
    "encoders = {}\n",
    "for col in cat_cols:\n",
    "    encoder =  OrdinalEncoder()\n",
    "    sales[[col]] = encoder.fit_transform(sales[[col]])\n",
    "    sales[col] = sales[col].astype(np.long)\n",
    "    encoders[col] = encoder\n",
    "    \n",
    "# change day column names to just day number\n",
    "train_day_cols = {col: col.split('_')[1] for col in sales.columns if col.startswith('d_')}\n",
    "sales.rename(columns=train_day_cols, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_day_cols = [str(num_train_val_days + 1 + o) for o in range(56)]\n",
    "for col in test_day_cols:\n",
    "    sales[col] = 0\n",
    "print(sales.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = pd.read_csv(data_dir/'sample_submission.csv')\n",
    "sample.tail(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_days = len(train_day_cols) + len(test_day_cols)\n",
    "num_stores = sales['store_id'].nunique()\n",
    "num_items = sales['item_id'].nunique()\n",
    "print('total days : ', num_days)\n",
    "print('num store_items - ', num_stores * num_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales['item_id'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv(data_dir/'calendar.csv')\\\n",
    "            .rename(columns={'d':'day'})\n",
    "\n",
    "cat_cal_cols = ['wm_yr_wk', 'weekday', 'wday', 'month', 'year',\n",
    "       'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2',\n",
    "       'snap_CA', 'snap_TX', 'snap_WI']\n",
    "# ignore_cal_cols = ['wm_yr_wk']\n",
    "\n",
    "for col in cat_cal_cols:\n",
    "    \n",
    "    # impute\n",
    "    if str(calendar[col].dtype)[:3] == 'obj':\n",
    "        fill_value = 'abcxyz' \n",
    "    elif str(calendar[col].dtype)[:3] == 'int':\n",
    "        fill_value = -1\n",
    "    calendar[[col]] = SimpleImputer(strategy='constant', fill_value=fill_value).fit_transform(calendar[[col]])\n",
    "    \n",
    "    # encode\n",
    "    if col not in encoders:\n",
    "        encoders[col] = OrdinalEncoder().fit(calendar[[col]])\n",
    "    calendar[[col]] = encoders[col].transform(calendar[[col]])\n",
    "    calendar[col] = calendar[col].astype(np.long)\n",
    "    \n",
    "# change day column names to just day number\n",
    "calendar['day'] = calendar['day'].apply(lambda x: x.split('_')[1])\n",
    "calendar['day'] = calendar['day'].astype(np.long)\n",
    "\n",
    "calendar.tail(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "prices = pd.read_csv(data_dir/'sell_prices.csv')\n",
    "for col in ['store_id', 'item_id', 'wm_yr_wk']:\n",
    "    prices[[col]] = encoders[col].transform(prices[[col]])\n",
    "    prices[col] = prices[col].astype(np.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices.sort_values('wm_yr_wk',ascending=False).head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales2 = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n",
    "                                       var_name='day', value_name='demand')\n",
    "sales2['day'] = sales2['day'].astype(np.long)\n",
    "\n",
    "sales2.sort_values('day', inplace=True)\n",
    "calendar.sort_values('day', inplace=True)\n",
    "\n",
    "sales2 = sales2.merge(calendar, on='day', how='left')\n",
    "sales2 = sales2.merge(prices, on=['store_id', 'item_id', 'wm_yr_wk'], how='left')\n",
    "sales2['sell_price'] = sales2['sell_price'].astype(np.float32)\n",
    "sales2['sell_price'] = sales2['sell_price'].fillna(0.0)\n",
    "\n",
    "sales2.sort_values(['item_id', 'store_id','day'], inplace=True)\n",
    "\n",
    "# scale continuous columns\n",
    "scalers = {}\n",
    "for col in ['sell_price','demand']:\n",
    "    scaler = MinMaxScaler()\n",
    "    sales2[[col]] = scaler.fit_transform(sales2[[col]])\n",
    "    scalers[col] = scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales2.to_parquet('combined.pq')\n",
    "with open('encoders.pkl','wb') as f:\n",
    "    pickle.dump(encoders,f)\n",
    "    \n",
    "with open('scalers.pkl','wb') as f:\n",
    "    pickle.dump(scalers, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sales2 = pd.read_parquet('combined.pq')\n",
    "print(sales2.shape)\n",
    "sales2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "x = torch.tensor(sales2[x_cat_cols + x_cont_cols].values)\n",
    "y = torch.tensor(sales2['demand'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales2[x_cat_cols + x_cont_cols].dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# from fastai v2\n",
    "def get_emb_size(nunique):\n",
    "    return min(600, round(1.6 * nunique**0.56))\n",
    "\n",
    "emb_sizes = [(sales2[col].nunique(), get_emb_size(sales2[col].nunique())) for col in x_cat_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('emb_sz.pkl','wb') as f:\n",
    "    pickle.dump(emb_sizes,f )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group_size = num_items * num_stores\n",
    "# group_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "num_features = x.size(1)\n",
    "x1 = x.view(-1, num_days, num_features).refine_names('item_store', 'day','features')\\\n",
    "        .align_to('day','item_store','features').contiguous()\n",
    "\n",
    "y1 = y.view(-1, num_days).refine_names('item_store', 'day')\\\n",
    "    .align_to('day', 'item_store').contiguous()\n",
    "\n",
    "print(f'x1.shape - {x1.shape} y1.shape - {y1.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "torch.save(x1.rename(None), 'x.pt')\n",
    "torch.save(y1.rename(None), 'y.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class M5DataSet(Dataset):\n",
    "    def __init__(self,x, y, src_len, tgt_len, bsz, dstype='train'):\n",
    "        assert dstype in ['train', 'test1', 'test2', 'val']\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.src_len = src_len\n",
    "        self.tgt_len = tgt_len\n",
    "        self.bsz = bsz\n",
    "        self.dstype = dstype\n",
    "        \n",
    "        self.val_days = self.src_len + self.tgt_len\n",
    "        self.test_days = self.tgt_len * 2\n",
    "        self.val_idx = self.x.size(0) - (self.val_days + self.test_days)\n",
    "        self.test1_idx = self.x.size(0) - (self.src_len + self.test_days)\n",
    "        print(f'val index - {self.val_idx}. test1_idx - {self.test1_idx}', )\n",
    "        \n",
    "    def __len__(self):\n",
    "        if self.dstype == 'train':\n",
    "            l = (self.x.size(0) - (self.src_len + self.val_days + self.test_days)) \n",
    "            return l\n",
    "        \n",
    "        if self.dstype =='val':\n",
    "             return 1\n",
    "        \n",
    "        if self.dstype == 'test1':\n",
    "            return 1\n",
    "        \n",
    "        return l\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.dstype == 'train':\n",
    "            # we have 30490 item_stores. We may not be able to load them all. So randomly pick bsz items. \n",
    "            item_store_mask = list(np.random.randint(0, self.x.size(1),(self.bsz,)))\n",
    "        elif self.dstype == 'val':\n",
    "            item_store_mask = list(np.random.randint(0, self.x.size(1),(self.bsz,)))\n",
    "            idx = self.val_idx\n",
    "        elif self.dstype == 'test1':\n",
    "            item_store_mask = list(np.arange(self.x.size(1)))\n",
    "            idx = self.test1_idx\n",
    "            print('test1 index - ', idx)\n",
    "        \n",
    "        x_src = self.x.rename(None)[idx:idx+self.src_len, item_store_mask, :]\n",
    "        x_tgt = self.x.rename(None)[idx+self.src_len:idx+self.src_len+self.tgt_len, item_store_mask, :]\n",
    "        y_src = self.y.rename(None)[idx:idx+self.src_len, item_store_mask]\n",
    "        y_tgt = self.y.rename(None)[idx+self.src_len:idx+self.src_len+self.tgt_len, item_store_mask]\n",
    "#         print(f'x.shape - {self.x.shape} y.shape - {self.y.shape} idx - {idx}. x_item.shape - {x_item.shape} y_item.shape - {y_item.shape}')\n",
    "        return x_src, x_tgt, y_src, y_tgt, item_store_mask\n",
    "\n",
    "# train_ds = M5DataSet(x1, y1, src_len, tgt_len, 200)\n",
    "# train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)\n",
    "    \n",
    "# def insert_embedding(inp, dim, index, emb):\n",
    "#     \"\"\"\n",
    "#     Replace columns with their embeddings. Works only with 2-d tensors.\n",
    "#     TODO - make it work for multi-dim tensors\n",
    "\n",
    "#     :param inp: tensor of two or more dimensions\n",
    "#     :param dim: dimension along which tensor should be expanded by inserting the embedding\n",
    "#     :param i: index of tensor along dim which is to be embedded\n",
    "#     :param emb: Embedding of shape [v,d], where v vocab_size and d is embedding dimension\n",
    "#     :return: \n",
    "#     \"\"\"\n",
    "#     # create a slice of the data to be replaced with embedding. \n",
    "#     s = inp.index_select(dim, torch.tensor([index])).squeeze(dim)\n",
    "#     embedded = emb(s.type(torch.long))\n",
    "    \n",
    "#     first_indices = torch.arange(0,index)\n",
    "#     last_indices = torch.arange(index+1,inp.size(dim))\n",
    "\n",
    "#     return torch.cat([inp.index_select(dim, first_indices), embedded.type(inp.dtype), inp.index_select(dim, last_indices)], axis=dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gx.shape - torch.Size([1969, 30490, 17]) gy.shape - torch.Size([1969, 30490])\n",
      "CPU times: user 8.16 ms, sys: 6.72 s, total: 6.73 s\n",
      "Wall time: 6.71 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "gx = torch.load('x.pt')\n",
    "gy = torch.load('y.pt')\n",
    "print(f'gx.shape - {gx.shape} gy.shape - {gy.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesModel(LightningModule):\n",
    "    def __init__(self, hparams):\n",
    "        super(SalesModel, self).__init__()\n",
    "        self.hparams = hparams\n",
    "        self.x_cat_cols = x_cat_cols\n",
    "        self.x_cont_cols = x_cont_cols\n",
    "        self.pos_encoder = PositionalEncoding(hparams.ninp, hparams.dropout)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(hparams.ninp, hparams.nhead, hparams.nhid, hparams.dropout)\n",
    "        decoder_layers = nn.TransformerDecoderLayer(hparams.ninp, hparams.nhead, hparams.nhid, hparams.dropout)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layers, hparams.nlayers)\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layers, hparams.nlayers)\n",
    "        \n",
    "#         self.lin = nn.Linear()\n",
    "#         self.transformer = nn.Transformer(d_model=hparams.ninp, nhead=hparams.nhead, \n",
    "#                                           num_encoder_layers=hparams.nlayers,\n",
    "#                                           num_decoder_layers=hparams.nlayers,\n",
    "#                                           dim_feedforward=hparams.nhid)\n",
    "        self.transformer = nn.Transformer(d_model=hparams.ninp,\n",
    "                                          custom_encoder=self.encoder, \n",
    "                                          custom_decoder = self.decoder)\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.lin1 = nn.Linear(hparams.ninp, 50)\n",
    "        self.lin2 = nn.Linear(50, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        print('reading data', flush=True)\n",
    "        self.x = gx\n",
    "        self.y = gy\n",
    "\n",
    "        with open('emb_sz.pkl','rb') as f:\n",
    "            emb_szs = pickle.load(f)\n",
    "        print(f'emb_szs - {emb_szs}')\n",
    "                    \n",
    "        self.embs = nn.ModuleList([nn.Embedding(e[0],e[1]) for e in emb_szs])\n",
    "#         self.init_weights()\n",
    "        \n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "#         self.src_embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.decoder.bias.data.zero_()\n",
    "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
    "        \n",
    "    @staticmethod\n",
    "    def add_model_specifi_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--bsz', default=20, type=int, help='batch_size', )\n",
    "        parser.add_argument('--src-len', default=90, type=int, help='source length')\n",
    "        parser.add_argument('--tgt-len', default=28, type=int, help='target length')\n",
    "        parser.add_argument('--ninp', default=320, type=int, help='expected features in the input')\n",
    "        parser.add_argument('--nhead', default=4, type=int, help='number of attention heads')\n",
    "        parser.add_argument('--nhid', default=256, type=int, help='dimesion of feed-forward network model')\n",
    "        parser.add_argument('--nlayers', default=2, type=int, help='number of encoder layers')\n",
    "        parser.add_argument('--dropout', default=0.2, type=float, help='dropout')\n",
    "        \n",
    "        # they are not hyper params, but adding them as pytorch lightening can save them\n",
    "        parser.add_argument('--num-cat-cols', default=len(x_cat_cols), type=int, help='number of categorical columns')\n",
    "        parser.add_argument('--num-cont-cols', default=len(x_cont_cols), type=int, help='number of numeric columns')\n",
    "        return parser\n",
    "    \n",
    "#     def _generate_square_subsequent_mask(self, sz):\n",
    "#         # populate the lower triangle with True and rest with False\n",
    "#         return torch.tril(torch.ones(sz, sz)) == 1.0\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "        \n",
    "    def train_dataloader(self):\n",
    "        train_ds = M5DataSet(self.x, self.y, self.hparams.src_len, self.hparams.tgt_len, self.hparams.bsz,dstype='train')  \n",
    "        print(f'train_ds.length - {len(train_ds)}')\n",
    "        train_dl = DataLoader(train_ds, batch_size=1, shuffle=True, pin_memory=True)\n",
    "        return train_dl\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        train_ds = M5DataSet(self.x, self.y, self.hparams.src_len, self.hparams.tgt_len, self.hparams.bsz,dstype='val')  \n",
    "        print(f'val.length - {len(train_ds)}')\n",
    "        train_dl = DataLoader(train_ds, batch_size=1, shuffle=False, pin_memory=True)\n",
    "        return train_dl\n",
    "\n",
    "    \n",
    "    def emb_lookups(self, xb, yb=None):\n",
    "        embs_t = []\n",
    "        for idx in range(self.hparams.num_cat_cols):\n",
    "#             print('looking up for ', idx)\n",
    "            embs_t.append(self.embs[idx](xb[:,:,idx].type(torch.long)))\n",
    "        xb_cat = torch.cat(embs_t, dim=2)\n",
    "        xb_cont = xb[:,:,self.hparams.num_cat_cols:]\n",
    "        \n",
    "        if yb is not None:\n",
    "            xb = torch.cat([xb_cat, xb_cont.type(xb_cat.dtype), yb.unsqueeze(2).type(xb_cat.dtype)], dim=2)\n",
    "        else:\n",
    "            xb = torch.cat([xb_cat, xb_cont.type(xb_cat.dtype)], dim=2)\n",
    "            \n",
    "        #pad to adjust the feature dimension\n",
    "        dim3_shortfall = self.hparams.ninp - xb.size(2)\n",
    "        assert dim3_shortfall >= 0\n",
    "        pad = nn.ConstantPad1d(padding=(0,dim3_shortfall),value=0)\n",
    "        xb = pad(xb) \n",
    "\n",
    "        return xb\n",
    "\n",
    "    def forward(self, x_src, y_src, x_tgt):        \n",
    "        x_src = self.emb_lookups(x_src, y_src)\n",
    "        x_tgt = self.emb_lookups(x_tgt)\n",
    "            \n",
    "        x_src = self.pos_encoder(x_src)\n",
    "#         print('shape after pos encoder - ', x_src.size())\n",
    "        out = self.transformer(x_src, x_tgt)\n",
    "#         print('shape after transformer - ', out.size())\n",
    "        out = self.relu(self.lin1(out))\n",
    "        out = self.sigmoid(self.lin2(out))\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def compute_loss(self, batch, batch_idx):\n",
    "        x_src, x_tgt, y_src, y_tgt, item_store_mask = batch\n",
    "        x_src = x_src.squeeze(0)\n",
    "        x_tgt = x_tgt.squeeze(0)\n",
    "        y_src = y_src.squeeze(0)\n",
    "        y_tgt = y_tgt.squeeze(0)\n",
    "        \n",
    "        yhat_tgt = self(x_src, y_src, x_tgt)\n",
    "        loss = self.criterion((yhat_tgt).reshape(-1).type(torch.float32), (y_tgt).reshape(-1).type(torch.float32))\n",
    "        return loss, y_tgt, yhat_tgt\n",
    "        \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, y_tgt, yhat_tgt = self.compute_loss(batch, batch_idx)\n",
    "        metrics = {'loss': loss, 'yhat_tgt_train_sum':yhat_tgt.sum().item()}\n",
    "\n",
    "        if batch_idx%10 == 0:\n",
    "#             print(f'{batch_idx} train loss: {loss}  yhat_tgt.sum: {yhat_tgt.sum().item()}  y_tgt.sum: {y_tgt.sum().item()}')\n",
    "#             wandb.log(metrics)   \n",
    "            self.logger.log_metrics(metrics)\n",
    "        return metrics    \n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y_tgt, yhat_tgt = self.compute_loss(batch, batch_idx)\n",
    "        metrics = {'val_loss': loss,'yhat_tgt_val_sum':yhat_tgt.sum().item()}  \n",
    "        if batch_idx%10 == 0:\n",
    "#             print(f'{batch_idx} val loss: {loss}  yhat_tgt.sum: {yhat_tgt.sum().item()}  y_tgt.sum: {y_tgt.sum().item()}')\n",
    "#             wandb.log(metrics)   \n",
    "            self.logger.log_metrics(metrics)\n",
    "        return metrics\n",
    "        \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=0.0001)\n",
    "        schedulers = [{\n",
    "             'scheduler': ReduceLROnPlateau(optimizer,patience=10, verbose=True),\n",
    "             'monitor': 'loss', # Default: val_loss\n",
    "             'interval': 'step',\n",
    "             'frequency': 1\n",
    "          }]\n",
    "        scheduler = ReduceLROnPlateau(optimizer,)\n",
    "#         scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, \n",
    "#                                                         max_lr=0.01, steps_per_epoch=len(data_loader), epochs=10)\n",
    "        return optimizer\n",
    "    \n",
    "#     def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx,\n",
    "#                        second_order_closure=None):\n",
    "#         optimizer.step()\n",
    "#         if batch_idx == 5:\n",
    "#             for name, param in model.named_parameters():\n",
    "#                 if param.requires_grad:\n",
    "#                     pass\n",
    "# #                     print(name, param.grad)\n",
    "#         optimizer.zero_grad()\n",
    "    \n",
    "#     def test(self):\n",
    "#         dl = self.test_dataloader()\n",
    "#         batch = next(iter(dl))\n",
    "#         return batch\n",
    "    \n",
    "    \n",
    "#     def test_dataloader(self):\n",
    "#         test_ds = M5DataSet(self.x, self.y, self.hparams.src_len, self.hparams.tgt_len, self.hparams.bsz, dstype='test1')  \n",
    "#         test_dl = DataLoader(test_ds, batch_size=1, shuffle=False)\n",
    "#         return test_dl\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "No environment variable for node rank defined. Set as 0.\n",
      "CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emb_szs - [(3049, 143), (7, 5), (3, 3), (10, 6), (3, 3), (7, 5), (7, 5), (12, 6), (6, 4), (31, 11), (5, 4), (5, 4), (3, 3), (2, 2), (2, 2), (2, 2)]\n"
     ]
    }
   ],
   "source": [
    "# bsz = 200\n",
    "# model = SalesModel(hparams)\n",
    "\n",
    "parser = ArgumentParser()\n",
    "parser = SalesModel.add_model_specifi_args(parser)\n",
    "hparams = parser.parse_args('--bsz 1000 --ninp 320 --nhid 512 --nlayers 1'.split())\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    filepath='models/weights.ckpt',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model = SalesModel(hparams)\n",
    "\n",
    "wandb_logger = WandbLogger(name='achinta',project='kaggle-m5-forecasting-accuracy')\n",
    "# run validation every 10 steps\n",
    "trainer = Trainer(gpus=1,max_epochs=1,auto_lr_find=False, val_check_interval=10,logger=wandb_logger)\n",
    "trainer.fit(model)\n",
    "trainer.save_checkpoint('models/weights_v1.1.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytorch_lightning.trainer.trainer.Trainer at 0x7f593abd4320>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = SalesModel.load_from_checkpoint('models/weights.ckpt')\n",
    "x_src, x_tgt, y_src, y_tgt, item_store_mask = model.test()\n",
    "x_src = x_src.squeeze(0)\n",
    "x_tgt = x_tgt.squeeze(0)\n",
    "y_src = y_src.squeeze(0)\n",
    "y_tgt = y_tgt.squeeze(0)\n",
    "print(f'x_src.shape - {x_src.shape}, x_tgt.shape - {x_tgt.shape} , y_src.shape - {y_src.shape} , y_tgt.shape - {y_tgt.shape}')\n",
    "\n",
    "model.eval()\n",
    "print('starting inference...')\n",
    "yhat_tgt = model(x_src, y_src, x_tgt)\n",
    "yhat_tgt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yhat_tgt = yhat_tgt.refine_names('days','item_store','demand')\n",
    "yhat_tgt_aligned = yhat_tgt.align_to('item_store','days','demand').squeeze(2).detach().numpy()\n",
    "print(f'yhat.shape: ', yhat_tgt_aligned.shape)\n",
    "\n",
    "# create preds df\n",
    "preds = pd.DataFrame()\n",
    "preds['id'] = sales['id']\n",
    "\n",
    "# read scalers\n",
    "with open('scalers.pkl','rb') as f:\n",
    "    scalers = pickle.load(f)\n",
    "\n",
    "\n",
    "pred_ids = preds['id'].tolist()\n",
    "# eval df should also be submitted (days 1942 to 1969)\n",
    "eval_ids = ['_'.join(o.split('_')[:5] + ['evaluation']) for o in pred_ids]\n",
    "eval_df = pd.DataFrame({'id': eval_ids})\n",
    "\n",
    "for idx in range(num_test1_days):\n",
    "    preds['F' + str(idx+1)] = yhat_tgt_aligned[:,idx]\n",
    "    preds['F' + str(idx+1)] = scalers['demand'].inverse_transform(preds[['F' + str(idx+1)]])\n",
    "    \n",
    "    eval_df['F' + str(idx+1)] = 0.0\n",
    "    \n",
    "out_df = pd.concat([preds,eval_df],axis=0)\n",
    "print(out_df.shape)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_df.to_csv('preds.csv', index=False)\n",
    "!kaggle competitions submit -c m5-forecasting-accuracy -f preds.csv -m \"transformers 3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !head preds.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_store_mask = list(np.random.randint(0, 10,3))\n",
    "item_store_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.randn(10).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(hparams)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.read_csv(data_dir/'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "eval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
